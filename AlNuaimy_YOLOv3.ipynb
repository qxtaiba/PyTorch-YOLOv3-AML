{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ExifTags\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from sys import platform\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "from . import torch_utils  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptedImageFormats = ['.bmp', '.jpg', '.jpeg', '.png', '.tif', '.tiff', '.dng']\n",
    "help_url = 'https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data'\n",
    "\n",
    "# find the orientation of the exif tag\n",
    "for orientation in ExifTags.TAGS.keys():\n",
    "    if ExifTags.TAGS[orientation] == 'Orientation':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images - Inference Dataset Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImages: \n",
    "    def __init__(self, path, imageSize = 416):\n",
    "        \n",
    "        # init files list  \n",
    "        files = []\n",
    "        # extract path \n",
    "        path = str(Path(path))  \n",
    "        \n",
    "        # check if path leads to a directory  and populate files list\n",
    "        if os.path.isdir(path):\n",
    "            files = sorted(glob.glob(os.path.join(path, '*.*')))\n",
    "        # check if path leads to a file and populate files list \n",
    "        elif os.path.isfile(path):\n",
    "            files = [path]\n",
    "\n",
    "        # extract image(s) if they are in the correct format \n",
    "        images = [x for x in files if os.path.splitext(x)[-1].lower() in acceptedImageFormats]\n",
    "        # extract number of images\n",
    "        numImages = len(images)\n",
    "        # init image size \n",
    "        self.imgSize = imageSize\n",
    "        # init files \n",
    "        self.files = images \n",
    "        # init number of files \n",
    "        self.numFiles = numImages \n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        # init count to zero \n",
    "        self.count = 0\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "\n",
    "        # check if we have loaded all of the images \n",
    "        if self.count == self.numFiles:\n",
    "            raise StopIteration\n",
    "        \n",
    "        # extract path\n",
    "        path = self.files[self.count]\n",
    "        # increment count\n",
    "        self.count += 1\n",
    "        # read image\n",
    "        img0 = cv2.imread(path)\n",
    "        # resize by adding padding\n",
    "        img = letterbox(img0, newShape = self.imgSize)[0]\n",
    "        # convert image from BGR to RGB and to 3x416x416\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return path, img, img0\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # return number of files\n",
    "        return self.numFiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images and Labels - Training and Testing Dataset Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get EXIF Size - Dataset Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# return the exif-corrected PIL size\n",
    "def getEXIFsize(img):\n",
    "\n",
    "    #extract image size\n",
    "    shape = img.size\n",
    "    try:        \n",
    "        # rotation by 270\n",
    "        if dict(img._getexif().items())[orientation] == 6:\n",
    "            shape = (shape[1], shape[0])\n",
    "        # rotation by 90\n",
    "        elif dict(img._getexif().items())[orientation] == 8:\n",
    "            shape = (shape[1], shape[0])    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image - Dataset Helper: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the following function is to load a single image from the COCO dataset. The function works by extracting the image path and reading it using the cv2.imread() method. The cv2.imread() method loads an image from the specified file. If the image cannot be read (because of missing file, improper permissions, unsupported or invalid format) then this method returns an empty matrix. The height and width of the image are then extracted and we calculate the resize factor such that we can resize the image to a preset image size. The resizing of the image is done using the cv2.resize() method with either bilinear interpolation, or alternatively a variation of a nearest-neighbour interpolation that resamples using the pixel area resolution. The function then returns the image, its height and width, as well as its resized height and width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(self, index):\n",
    "\n",
    "    # extract path \n",
    "    path = self.imgFiles[index]\n",
    "    # read image \n",
    "    img = cv2.imread(path)  \n",
    "    # extract height and width of image \n",
    "    originalHeight, originalWidth = img.shape[:2]  \n",
    "    # resize factor so that we can resize image to imageSize\n",
    "    resizeFactor = self.imageSize / max(originalHeight, originalWidth)\n",
    "    \n",
    "    # always resize down, only resize up if training with augmentation\n",
    "    if resizeFactor != 1:\n",
    "        # interpolate image\n",
    "        interp = cv2.INTER_AREA if resizeFactor < 1 and not self.isAugment else cv2.INTER_LINEAR\n",
    "        # resize image\n",
    "        img = cv2.resize(img, (int(originalWidth * resizeFactor), int(originalHeight * resizeFactor)), interpolation = interp)\n",
    "    \n",
    "    # extract height and width of resized image \n",
    "    resizedHeight, resizedWidth = img.shape[:2]\n",
    "\n",
    "    return img, (originalHeight, originalWidth), (resizedHeight, resizedWidth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment HSV - Dataset Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to modify an input image in-place by manipulating its hue, saturation, and value.  \n",
    "\n",
    "  \n",
    "It is important to note that hue, saturation, and value are the main colour properties that allow us to distinguish between different colours. Modifying these values allows us to augment our input image, expand out dataset, and improve our training results.\n",
    "\n",
    "\n",
    "1. Hues are the three primary colours (red, blue, and yellow) and the three secondary colours (orange, green, and violet) that appear in the colour wheel or colour circle. When you refer to hue, you are referring to pure colour, or the visible spectrum of basic colours that can be seen in a rainbow. \n",
    "\n",
    "\n",
    "2. Colour saturation is the purity and intensity of a colour as displayed in an image. The higher the saturation of a colour, the more vivid and intense it is. The lower a colour’s saturation, or chroma, the closer it is to pure grey on the grayscale.\n",
    "\n",
    "\n",
    "3. Colour value refers to the relative lightness or darkness of a colour. We perceive colour value based on the quantity of light reflected off of a surface and absorbed by the human eye. We refer to the intensity of the light that reaches the eye as “luminance.”\n",
    "\n",
    "\n",
    "The cv2.LUT() method allows us to create a lookup-table with randomly generated values that are used to  transform the image’s hue, saturation, and value to new values. The image is then modified using cv2.cvtColor() to to convert the image from one colour space to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentHSV(img, hgain = 0.5, sgain = 0.5, vgain = 0.5):\n",
    "    \n",
    "    # init random gains\n",
    "    randomGains = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  \n",
    "    # extract hue, saturation, value from image\n",
    "    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
    "    # init numpy array\n",
    "    x = np.arange(0, 256, dtype = np.int16)\n",
    "    # init look-up table for hue with random gain\n",
    "    lookUpHue = ((x * randomGains[0]) % 180).astype(img.dtype)\n",
    "    # init look-up table for saturation with random gain \n",
    "    lookUpSat = np.clip(x * randomGains[1], 0, 255).astype(img.dtype)\n",
    "    # init look-up table for value with random gain\n",
    "    lookUpVal = np.clip(x * randomGains[2], 0, 255).astype(img.dtype)\n",
    "    # extract new hue, saturation, value for image using look-up tables\n",
    "    modifiedHSV = cv2.merge((cv2.LUT(hue, lookUpHue), cv2.LUT(sat, lookUpSat), cv2.LUT(val, lookUpVal))).astype(img.dtype)\n",
    "    # modify image\n",
    "    cv2.cvtColor(modifiedHSV, cv2.COLOR_HSV2BGR, dst = img)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mosaic - Dataset Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to load images into a mosaic of four. It is a form of augmentation that is used only during training and it works by taking a total of four images, creating a base image with the corresponding number of tiles, and then calculating the position of each image on the base image. It also calculates the required padding, normalises the image labels, and then concatenates/clips the labels and applies an augmentation to both the images and labels, and returns them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic(self, index):   \n",
    "\n",
    "    # init labels \n",
    "    mosaicLabels = []\n",
    "    # extract image size\n",
    "    imageSize = self.imageSize\n",
    "    # randomly init center coordinates\n",
    "    centerX, centerY = [int(random.uniform(imageSize * 0.5, imageSize * 1.5)) for _ in range(2)]\n",
    "    # randomly init an additional three image indices\n",
    "    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]\n",
    "    \n",
    "    for i, imageIndex in enumerate(indices):\n",
    "        # load current image\n",
    "        img, (originalHeight, originalWidth), (resizedHeight, resizedWidth) = loadImage(self, imageIndex)\n",
    "\n",
    "        if i == 0: # top left\n",
    "            # create base image with 4 tiles\n",
    "            baseImage = np.full((imageSize * 2, imageSize * 2, img.shape[2]), 114, dtype = np.uint8)  \n",
    "            # xmin, ymin, xmax, ymax for large image\n",
    "            xMinlarge, yminLarge, xMaxLarge, yMaxLarge = max(centerX - resizedWidth, 0), max(centerY - resizedHeight, 0), centerX, centerY\n",
    "            # xmin, ymin, xmax, ymax for small image\n",
    "            xMinSmall, yMinSmall, xMaxSmall, yMaxSmall = resizedWidth - (xMaxLarge - xMinlarge), resizedHeight - (yMaxLarge - yminLarge), resizedWidth, resizedHeight  \n",
    "        \n",
    "        elif i == 1:  # top right\n",
    "            # xmin, ymin, xmax, ymax for large image\n",
    "            xMinlarge, yminLarge, xMaxLarge, yMaxLarge = centerX, max(centerY - resizedHeight, 0), min(centerX + resizedWidth, imageSize * 2), centerY\n",
    "            # xmin, ymin, xmax, ymax for small image\n",
    "            xMinSmall, yMinSmall, xMaxSmall, yMaxSmall = 0, resizedHeight - (yMaxLarge - yminLarge), min(resizedWidth, xMaxLarge - xMinlarge), resizedHeight\n",
    "        \n",
    "        elif i == 2: # bottom left\n",
    "            # xmin, ymin, xmax, ymax for large image\n",
    "            xMinlarge, yminLarge, xMaxLarge, yMaxLarge = max(centerX - resizedWidth, 0), centerY, centerX, min(imageSize * 2, centerY + resizedHeight)\n",
    "            # xmin, ymin, xmax, ymax for small image\n",
    "            xMinSmall, yMinSmall, xMaxSmall, yMaxSmall = resizedWidth - (xMaxLarge - xMinlarge), 0, max(centerX, resizedWidth), min(yMaxLarge - yminLarge, resizedHeight)\n",
    "        \n",
    "        elif i == 3: # bottom right\n",
    "            # xmin, ymin, xmax, ymax for large image\n",
    "            xMinlarge, yminLarge, xMaxLarge, yMaxLarge = centerX, centerY, min(centerX + resizedWidth, imageSize * 2), min(imageSize * 2, centerY + resizedHeight)\n",
    "            # xmin, ymin, xmax, ymax for small image\n",
    "            xMinSmall, yMinSmall, xMaxSmall, yMaxSmall = 0, 0, min(resizedWidth, xMaxLarge - xMinlarge), min(yMaxLarge - yminLarge, resizedHeight)\n",
    "\n",
    "        # init base image parameters \n",
    "        baseImage[yminLarge:yMaxLarge, xMinlarge:xMaxLarge] = img[yMinSmall:yMaxSmall, xMinSmall:xMaxSmall]  \n",
    "        \n",
    "        # calculate padding \n",
    "        widthPadding = xMinlarge - xMinSmall\n",
    "        heightPadding = yminLarge - yMinSmall\n",
    "\n",
    "        # extract labels\n",
    "        labels = self.labels[imageIndex]\n",
    "        _labels = labels.copy()\n",
    "\n",
    "        # normalize xywh to xyxy format\n",
    "        if labels.size > 0:\n",
    "            _labels[:, 1] = resizedWidth * (labels[:, 1] - labels[:, 3] / 2) + widthPadding\n",
    "            _labels[:, 2] = resizedHeight * (labels[:, 2] - labels[:, 4] / 2) + heightPadding\n",
    "            _labels[:, 3] = resizedWidth * (labels[:, 1] + labels[:, 3] / 2) + widthPadding\n",
    "            _labels[:, 4] = resizedHeight * (labels[:, 2] + labels[:, 4] / 2) + heightPadding\n",
    "        \n",
    "        mosaicLabels.append(_labels)\n",
    "\n",
    "    # check if mosaicLabels is not empty\n",
    "    if len(mosaicLabels):\n",
    "        # concatenate labels\n",
    "        mosaicLabels = np.concatenate(mosaicLabels, 0)\n",
    "        # clip labels\n",
    "        np.clip(mosaicLabels[:, 1:], 0, 2 * imageSize, out = mosaicLabels[:, 1:])\n",
    "\n",
    "    # augment images and labels\n",
    "    baseImage, mosaicLabels = randAffine(baseImage, mosaicLabels,degrees = self.hyp['degrees'], translate = self.hyp['translate'], scale = self.hyp['scale'], shear = self.hyp['shear'], border = -imageSize // 2)  # border to remove\n",
    "\n",
    "    return baseImage, mosaicLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letterbox - Dataset Helper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to resize an input image into a 32-pixel-multiple rectangle. This reduces  the inference time proportionally to the amount of letterboxed area padded onto a square image. It works by extracting the current shape, calculating the necessary padding, resizing it if necessary, and then creating and adding a border using the cv2.copyMakeBorder() method. It returns the letterboxed image, the scaling ratio, as well as the padding used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, newShape = (416, 416), color = (114, 114, 114), auto = True, scaleFill = False, scaleup = True):\n",
    "\n",
    "    # extract current image shape\n",
    "    currShape = img.shape[:2]\n",
    "\n",
    "    # check if new image shape is an integer or a tuple\n",
    "    if isinstance(newShape, int):\n",
    "        # create tuple\n",
    "        newShape = (newShape, newShape)\n",
    "    \n",
    "    # calculate scale ratio by dividing new shape by old shape\n",
    "    scaleRatio = min(newShape[0] / currShape[0], newShape[1] / currShape[1])\n",
    "\n",
    "    # only scale down, do not scale up\n",
    "    if not scaleup:\n",
    "        scaleRatio = min(scaleRatio, 1.0)\n",
    "\n",
    "    # extract unpadded shape\n",
    "    unpaddedShape = (int(round(currShape[1] * scaleRatio)), int(round(currShape[0] * scaleRatio)))\n",
    "    # calculate width and height padding \n",
    "    widthPadding, heightPadding = newShape[1] - unpaddedShape[0], newShape[0] - unpaddedShape[1]  \n",
    "    \n",
    "    if auto:  \n",
    "        widthPadding, heightPadding = np.mod(widthPadding, 32), np.mod(heightPadding, 32)  # wh padding\n",
    "\n",
    "    # resize image if current shape does not equal the unpadded shape \n",
    "    if currShape[::-1] != unpaddedShape:\n",
    "        img = cv2.resize(img, unpaddedShape, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    # divide width padding into two sides (left/right)\n",
    "    widthPadding /= 2\n",
    "    # divide height padding into two side (bottom/above)\n",
    "    heightPadding /= 2\n",
    "    # create top/bottom border\n",
    "    top, bottom = int(round(heightPadding - 0.1)), int(round(heightPadding + 0.1))\n",
    "    # create left/right border \n",
    "    left, right = int(round(widthPadding - 0.1)), int(round(widthPadding + 0.1))\n",
    "    # add borders\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value = color)  \n",
    "\n",
    "    return img, (scaleRatio, scaleRatio), (widthPadding, heightPadding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Affine - Dataset Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is another form of dataset augmentation used to apply rotate, scale, translate, and shear transforms to an input image. It also transforms the label coordinates and returns the image and label coordinates. \n",
    "\n",
    "It works by creating and initialising a rotate/scale matrix, a translate matrix, and a shear matrix. These matrices are then combined to create a combined rotation matrix. \n",
    "\n",
    "The function then checks if the image needs to be changed and applies the cv2.warpAffine method with linear interpolation. This function applies an affine transformation to the image. It then transforms the label coordinates by first warping the coordinates, creating new boxes and filtering out warped points outside of the image bounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randAffine(img, targets =(), degrees = 10, translate =.1, scale =.1, shear = 10, border = 0):\n",
    "\n",
    "    constVal = 1e-16\n",
    "    \n",
    "    # calculate height\n",
    "    height = img.shape[0] + border * 2\n",
    "    # calculate width \n",
    "    width = img.shape[1] + border * 2\n",
    "\n",
    "    # rotate and scale by first creating 3x3 identity matrix \n",
    "    R = np.eye(3)\n",
    "    # init random angle value \n",
    "    angle = random.uniform(-degrees, degrees)\n",
    "    # init random scale valye \n",
    "    scale = random.uniform(1 - scale, 1 + scale)\n",
    "    # create rotation matrix\n",
    "    R[:2] = cv2.getRotationMatrix2D(angle = angle, center =(img.shape[1] / 2, img.shape[0] / 2), scale = scale)\n",
    "\n",
    "    # translate by first creating 3x3 identity matrix\n",
    "    T = np.eye(3)\n",
    "    # init x translation\n",
    "    T[0, 2] = random.uniform(-translate, translate) * img.shape[0] + border  \n",
    "    # init y translation\n",
    "    T[1, 2] = random.uniform(-translate, translate) * img.shape[1] + border  \n",
    "\n",
    "    # shear by first creating 3x3 identity matrix\n",
    "    S = np.eye(3)\n",
    "    # init x shear [deg]\n",
    "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  \n",
    "    # init y shear [deg] \n",
    "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  \n",
    "\n",
    "    # creatte combined rotation matrix\n",
    "    M = S @ T @ R\n",
    "\n",
    "    # check if image changed\n",
    "    if (border != 0) or (M != np.eye(3)).any():\n",
    "        img = cv2.warpAffine(img, M[:2], dsize = (width, height), flags = cv2.INTER_LINEAR, borderValue = (114, 114, 114))\n",
    "\n",
    "    # transform label coordinates\n",
    "    if len(targets):\n",
    "\n",
    "        # warp points\n",
    "        xy = np.ones((len(targets) * 4, 3))\n",
    "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(len(targets) * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "        xy = (xy @ M.T)[:, :2].reshape(len(targets), 8)\n",
    "\n",
    "        # create new boxes\n",
    "        x = xy[:, [0, 2, 4, 6]]\n",
    "        y = xy[:, [1, 3, 5, 7]]\n",
    "        xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, len(targets)).T\n",
    "\n",
    "        # reject warped points outside of image\n",
    "        xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n",
    "        xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n",
    "        \n",
    "        # extract width\n",
    "        w = xy[:, 2] - xy[:, 0]\n",
    "        # extract height\n",
    "        h = xy[:, 3] - xy[:, 1]\n",
    "\n",
    "        # calculate area \n",
    "        area = w * h\n",
    "        area0 = (targets[:, 3] - targets[:, 1]) * (targets[:, 4] - targets[:, 2])\n",
    "        \n",
    "        # calculate aspect ratio\n",
    "        aspectRatio = np.maximum(w/(h + constVal), h/(w + constVal))  \n",
    "        \n",
    "        i = (w > 4) & (h > 4) & (area / (area0 * scale + constVal) > 0.2) & (aspectRatio < 10)\n",
    "\n",
    "        targets = targets[i]\n",
    "        targets[:, 1:5] = xy[i]\n",
    "\n",
    "    return img, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set printoptions\n",
    "torch.set_printoptions(linewidth = 320, precision = 5, profile ='long')\n",
    "np.set_printoptions(linewidth = 320, formatter ={'float_kind': '{:11.5g}'.format})  # format short g, %precision = 5\n",
    "matplotlib.rc('font', **{'size': 11})\n",
    "\n",
    "# prevent OpenCV from multithreading in order to use PyTorch DataLoader\n",
    "cv2.setNumThreads(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert boxes from [x1, y1, x2, y2] to [x, y, w, h] where x1, y1 represents the top-left coordinates and x2, y2 represents the bottom-right\n",
    "def xyxy2xywh(x):\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # center x coordinate\n",
    "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # center y coordinate\n",
    "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
    "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1 = top-left, xy2 = bottom-right\n",
    "def xywh2xyxy(x):\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top-left x coordinate\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top-left y coordinate\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom-right x coordinate\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom-right y coordinate\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale coordinates from first shape to that of second shape \n",
    "def scaleCoordinates(img1_shape, coords, img0_shape, ratio_pad = None):\n",
    "    \n",
    "    # check if ratioPad is set to None\n",
    "    if ratio_pad is None:  \n",
    "        # calculate gain (old/new) from img0_shape\n",
    "        gain = max(img1_shape) / max(img0_shape) \n",
    "        # calculate width and height padding from img0_shape\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2 \n",
    "    \n",
    "    else:\n",
    "        # calculate gain from ratioPad\n",
    "        gain = ratio_pad[0][0]\n",
    "        # calculate padding from ratioPad \n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    # extract x and y padding valyes \n",
    "    xPadding, yPadding = pad[0], pad[1]\n",
    "\n",
    "    # rescale coordinates\n",
    "    coords[:, [0, 2]] -= xPadding \n",
    "    coords[:, [1, 3]] -= yPadding  \n",
    "    coords[:, :4] /= gain\n",
    "\n",
    "    # clip bounding xyxy bounding boxes to image shape (height, width)\n",
    "    coords[:, 0].clamp_(0, img0_shape[1])  # x1\n",
    "    coords[:, 1].clamp_(0, img0_shape[0])  # y1\n",
    "    coords[:, 2].clamp_(0, img0_shape[1])  # x2\n",
    "    coords[:, 3].clamp_(0, img0_shape[0])  # y2\n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we want to create a precision-recall curve and compute the average precision for each class\n",
    "# F1 score is (harmonic mean of precision and recall)\n",
    "def getAPClass(truePositives, objectnessVal, predictedClasses, targetClasses):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:    True positives (nparray, nx1 or nx10).\n",
    "        conf:  Objectness value from 0-1 (nparray).\n",
    "        pred_cls: Predicted object classes (nparray).\n",
    "        target_cls: True object classes (nparray).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # sort by objectness and store sorted indices in objectnessSortIndices\n",
    "    objectnessSortIndices = np.argsort(-objectnessVal)\n",
    "    # sort truePositive using sorted indices\n",
    "    truePositives = truePositives[objectnessSortIndices]\n",
    "    # sort objectnessVal using sorted indices\n",
    "    objectnessVal = objectnessVal[objectnessSortIndices]\n",
    "    # sort predClasses using sorted indices\n",
    "    predictedClasses = predictedClasses[objectnessSortIndices]\n",
    "    # find all unique classes \n",
    "    uniqueClasses = np.unique(targetClasses)\n",
    "    # init constant val\n",
    "    constVal = 1e-16\n",
    "\n",
    "    # score to evaluate P and R https://github.com/ultralytics/yolov3/issues/898\n",
    "    precisionScore = 0.1  \n",
    "    shape = [uniqueClasses.shape[0], truePositives.shape[1]]  # number class, number iou thresholds (i.e. 10 for mAP0.5...0.95)\n",
    "    AP, precision, recall = np.zeros(shape), np.zeros(shape), np.zeros(shape)\n",
    "\n",
    "    # iterate through each class stored in unique classes\n",
    "    for classIndex, uniqueClass in enumerate(uniqueClasses):\n",
    "        objectnessSortIndices = predictedClasses == uniqueClass\n",
    "        # find number of ground truth objects\n",
    "        numGroundTruthObjects = (targetClasses == uniqueClass).sum() \n",
    "        # find number of predicted objects \n",
    "        numPredictedObjects = objectnessSortIndices.sum()  \n",
    "\n",
    "        # if there are no predicted objects AND no ground truth objects then we just skip this loop \n",
    "        if numPredictedObjects == 0 or numGroundTruthObjects == 0:\n",
    "            continue\n",
    "        \n",
    "        # otherwise if both number of predicted objects and number of ground truth objects are both non-zero\n",
    "        else:\n",
    "            # find the cumulative sum of false positives \n",
    "            cumulativeFalsePositives = (1 - truePositives[objectnessSortIndices]).cumsum(0)\n",
    "            # find the cumulative sum of true positives\n",
    "            cumulativeTruePositives = truePositives[objectnessSortIndices].cumsum(0)\n",
    "\n",
    "            # create the recall curve and append it to list\n",
    "            recallCurve = cumulativeTruePositives / (numGroundTruthObjects + constVal)  \n",
    "            # calculate recall at precisionScore\n",
    "            recall[classIndex] = np.interp(-precisionScore, -objectnessVal[objectnessSortIndices], recallCurve[:, 0]) \n",
    "\n",
    "            # create the precision curve and append it to list\n",
    "            precisionCurve = cumulativeTruePositives / (cumulativeTruePositives + cumulativeFalsePositives)  \n",
    "            # calculate precision at precisionScore\n",
    "            precision[classIndex] = np.interp(-precisionScore, -objectnessVal[objectnessSortIndices], precisionCurve[:, 0]) \n",
    "\n",
    "            # calculate AP from recall-precision curve\n",
    "            for j in range(truePositives.shape[1]):\n",
    "                AP[classIndex, j] = getAP(recallCurve[:, j], precisionCurve[:, j])\n",
    "\n",
    "    # calculate F1 score\n",
    "    F1 = 2 * precision * recall / (precision + recall + constVal)\n",
    "\n",
    "    return precision, recall, AP, F1, uniqueClasses.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAP(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # append sentinel values at the beginning and end of the recall curve and precision curve\n",
    "    mrec = np.concatenate(([0.], recall, [min(recall[-1] + 1E-3, 1.)]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "    # calculate the precision envelope\n",
    "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
    "    # init a 101-point interp (COCO)\n",
    "    x = np.linspace(0, 1, 101)\n",
    "    # integrate area under envelope to calculate average precision\n",
    "    AP = np.trapz(np.interp(x, mrec, mpre), x)\n",
    "\n",
    "    return AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
    "def boundingBoxIOU(firstBox, secondBox, x1y1x2y2 = True, GIoU = False):\n",
    "    \n",
    "    # transpose secondBox\n",
    "    secondBox = secondBox.t()\n",
    "    # init const val \n",
    "    constVal = 1e-16\n",
    "\n",
    "    if x1y1x2y2:\n",
    "        # extract coordinates of bounding boxes - transform from center and width to exact coordinates\n",
    "        firstBoxX1, firstBoxY1 = firstBox[0], firstBox[1]\n",
    "        firstBoxX2, firstBoxY2 = firstBox[2], firstBox[3]\n",
    "        secondBoxX1, secondBoxY1 = secondBox[0], secondBox[1]\n",
    "        secondBoxX2, secondBoxY2 = secondBox[2], secondBox[3]\n",
    "\n",
    "    else:  \n",
    "        # extract coordinates of bounding boxes - transform from xywh to xyxy\n",
    "        firstBoxX1, firstBoxX2 = firstBox[0] - firstBox[2] / 2, firstBox[0] + firstBox[2] / 2\n",
    "        firstBoxY1, firstBoxY2 = firstBox[1] - firstBox[3] / 2, firstBox[1] + firstBox[3] / 2\n",
    "        secondBoxX1, secondBoxX2 = secondBox[0] - secondBox[2] / 2, secondBox[0] + secondBox[2] / 2\n",
    "        secondBoxY1, secondBoxY2 = secondBox[1] - secondBox[3] / 2, secondBox[1] + secondBox[3] / 2\n",
    "\n",
    "    # extract intersection rectangle coordinates\n",
    "    rectIntersectionX1, rectIntersectionY1  = torch.max(firstBoxX1, secondBoxX1), torch.max(firstBoxY1, secondBoxY1) \n",
    "    rectIntersectionX2, rectIntersectionY2 = torch.min(firstBoxX2, secondBoxX2), torch.min(firstBoxY2, secondBoxY2)\n",
    "    \n",
    "    # calculate intersection width\n",
    "    intersectionWidth = (rectIntersectionX2 - rectIntersectionX1).clamp(0)\n",
    "    # calculate intersection height\n",
    "    intersectionHeight = (rectIntersectionY2 - rectIntersectionY1).clamp(0)\n",
    "    # calculate intersection area \n",
    "    intersectionArea = intersectionWidth * intersectionHeight\n",
    "\n",
    "    # calculate width and height of first box \n",
    "    firstWidth, firstHeight = firstBoxX2 - firstBoxX1, firstBoxY2 - firstBoxY1\n",
    "    # calculate width and height of second box \n",
    "    secondWidth, secondHeight = secondBoxX2 - secondBoxX1, secondBoxY2 - secondBoxY1\n",
    "    # calculate union area \n",
    "    unionArea = (firstWidth * firstHeight + constVal) + secondWidth * secondHeight - intersectionArea\n",
    "\n",
    "    # calculate intersection-over-union (IoU) area\n",
    "    iou = intersectionArea / unionArea  \n",
    "    \n",
    "    # check if GIoU is true \n",
    "    if GIoU:\n",
    "        # extract smallest enclosing width (convex width)\n",
    "        smallestEnclosingWidth = torch.max(firstBoxX2, secondBoxX2) - torch.min(firstBoxX1, secondBoxX1)  \n",
    "        # extract smallest enclosing height (convex height)\n",
    "        smallestEnclosingHeight = torch.max(firstBoxY2, secondBoxY2) - torch.min(firstBoxY1, secondBoxY1) \n",
    "        # calculate smallest enclosing area (convex araea) \n",
    "        smallestEnclosingArea = smallestEnclosingWidth * smallestEnclosingHeight + constVal \n",
    "        \n",
    "        # return GIoU\n",
    "        return iou - (smallestEnclosingArea - unionArea) / smallestEnclosingArea  \n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxIOU(box1, box2):\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate width and height of first box\n",
    "    boxOneWidth = box1.t()[2] - box1.t()[0]\n",
    "    boxOneHeight = box1.t()[3] - box1.t()[1]\n",
    "    # calculate width and height of second box \n",
    "    boxTwoWdith = box2.t()[2] - box2.t()[0]\n",
    "    boxTwoHeight = box2.t()[3] - box2.t()[1]\n",
    "    # calculate area of first box\n",
    "    areaOne = boxOneWidth * boxOneHeight\n",
    "    # calculate area of second box \n",
    "    areaTwo = boxTwoWdith * boxTwoHeight\n",
    "\n",
    "    # calculate intersection area \n",
    "    intersectionArea = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    # calculate union area \n",
    "    unionArea = (areaOne[:, None] + areaTwo - intersectionArea)\n",
    "\n",
    "    return intersectionArea /  unionArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2\n",
    "def widthHeightIOU(firstWidthHeight, secondWidthHeight):\n",
    "    \n",
    "    # extract shapes \n",
    "    firstWidthHeight = firstWidthHeight[:, None]  # [N,1,2]\n",
    "    secondWidthHeight = secondWidthHeight[None]  # [1,M,2]\n",
    "    # caclulate intersection area \n",
    "    intersectionArea = torch.min(firstWidthHeight, secondWidthHeight).prod(2)  # [N,M]\n",
    "    # calculate union area \n",
    "    unionArea = (firstWidthHeight.prod(2) + secondWidthHeight.prod(2) - intersectionArea) \n",
    "\n",
    "    return intersectionArea / unionArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLosses(predictions, targets, model):  \n",
    "    # init float tensor depending on cuda availability \n",
    "    FloatTensor = torch.cuda.FloatTensor if predictions[0].is_cuda else torch.Tensor\n",
    "\n",
    "    # init class loss tensor to zeroes\n",
    "    classLoss = FloatTensor([0])\n",
    "    # init box loss tensor to zeroes \n",
    "    GIoUBoxLoss = FloatTensor([0])\n",
    "    # init object loss tensor to zeroes \n",
    "    objectLoss = FloatTensor([0])\n",
    "\n",
    "    # calculate and extract targets \n",
    "    tcls, tbox, indices, anchors = buildTargets(predictions, targets, model)  \n",
    "\n",
    "    # define criteria for BCE loss\n",
    "    BCEcls = nn.BCEWithLogitsLoss(pos_weight = FloatTensor([model.hyp['cls_pw']]), reduction = 'mean')\n",
    "    BCEobj = nn.BCEWithLogitsLoss(pos_weight = FloatTensor([model.hyp['obj_pw']]), reduction = 'mean')\n",
    "\n",
    "    # init total number of targets to zero \n",
    "    cumNumTargets = 0  \n",
    "    \n",
    "    # iterate through each layer predection (output )\n",
    "    for layerIdx, layerPrediction in enumerate(predictions):\n",
    "        # extract image index, anchor, y grid coordinate, x grid coordinate \n",
    "        imageIndex, anchor, gridY, gridX = indices[layerIdx]  \n",
    "        # init target objectness value to tensor of zeroes \n",
    "        targetObj = torch.zeros_like(layerPrediction[..., 0])  \n",
    "        # extract number of targets \n",
    "        numTargets = imageIndex.shape[0]  \n",
    "\n",
    "        # check if number of targets is larger than zero \n",
    "        if numTargets:\n",
    "            # increment cumulative number of targets with current number of targets \n",
    "            cumNumTargets += numTargets  \n",
    "            # extract prediction subset corresponding to current targets\n",
    "            predictionSubset = layerPrediction[imageIndex, anchor, gridY, gridX]  \n",
    "\n",
    "            # extract prediction x, y coordinates \n",
    "            predictionXY = predictionSubset[:, :2].sigmoid()\n",
    "            # extract prediction w,h values \n",
    "            predictionWH = predictionSubset[:, 2:4].exp().clamp(max = 1E3) * anchors[layerIdx]\n",
    "            # create predicted boz by concatenating predictionXY and predictionWH\n",
    "            predictedBox = torch.cat((predictionXY, predictionWH), 1) \n",
    "            # calculate GIoU\n",
    "            GIoU = boundingBoxIOU(predictedBox.t(), tbox[layerIdx], x1y1x2y2 = False, GIoU = True) \n",
    "            # calculate GIoU box loss \n",
    "            GIoUBoxLoss += (1.0 - GIoU).mean()  \n",
    "\n",
    "            # calculate objectness value (GIoU ratio)\n",
    "            targetObj[imageIndex, anchor, gridY, gridX] = (1.0 - model.gr) + model.gr * GIoU.detach().clamp(0).type(targetObj.dtype)  \n",
    "\n",
    "            # calculate and sum BCE class loss\n",
    "            _targets = torch.full_like(predictionSubset[:, 5:], 0.0)  \n",
    "            _targets[range(numTargets), tcls[layerIdx]] = 1.0\n",
    "            classLoss += BCEcls(predictionSubset[:, 5:], _targets)  \n",
    "\n",
    "        # calculate and sum object loss \n",
    "        objectLoss += BCEobj(layerPrediction[..., 4], targetObj) \n",
    "\n",
    "    # finalise values for GIoU box loss using hyperparameters\n",
    "    GIoUBoxLoss *= model.hyp['giou']\n",
    "    # finalise values for object loss using hyperparameters\n",
    "    objectLoss *= model.hyp['obj']\n",
    "    # finalise values for class loss using hyperparameters\n",
    "    classLoss *= model.hyp['cls']\n",
    "\n",
    "    # calculate total loss \n",
    "    totLoss = GIoUBoxLoss + objectLoss + classLoss\n",
    "\n",
    "    return totLoss, torch.cat((GIoUBoxLoss, objectLoss, classLoss, totLoss)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build targets for getLosses(), input targets(image,class,x,y,w,h)\n",
    "def buildTargets(prediction, targets, model):\n",
    "    \n",
    "    # extract number of targets \n",
    "    numTargets = targets.shape[0]\n",
    "    # init target classes, target boxes, target indices, target anchors to empty lists\n",
    "    targetClasses, targetBoxes, targetIndices, targetAnchors = [], [], [], []\n",
    "    # init gain to tensor filled with ones \n",
    "    gain = torch.ones(6, device = targets.device)\n",
    "\n",
    "    # iterate through each layer in YOLO's layers\n",
    "    for idx, layer in enumerate(model.yoloLayers):\n",
    "        # extract anchors in current layer \n",
    "        anchors = model.moduleList[layer].anchorVector\n",
    "        # extract number of anchors\n",
    "        numAnchors = anchors.shape[0]  \n",
    "        # create anchor tensor \n",
    "        anchorTensor = torch.arange(numAnchors).view(numAnchors, 1).repeat(1, numTargets)  \n",
    "        # calculate xyxy gain\n",
    "        gain[2:] = torch.tensor(prediction[idx].shape)[[3, 2, 3, 2]] \n",
    "\n",
    "        # match targets to anchors\n",
    "        \n",
    "        # init layer anchor indices list \n",
    "        layerAnchorIndices = []\n",
    "        # calculate scaled targets by multiplying by gain \n",
    "        scaledTargets = targets*gain\n",
    "        # init offsets\n",
    "        offsets = 0\n",
    "\n",
    "        # check if number of targets is larger than zero \n",
    "        if numTargets:\n",
    "            layer = widthHeightIOU(anchors, scaledTargets[:, 4:6]) > model.hyp['iou_t']  \n",
    "            layerAnchorIndices, scaledTargets = anchorTensor[layer], scaledTargets.repeat(numAnchors, 1, 1)[layer]  #\n",
    "            # overlaps\n",
    "            gridXY = scaledTargets[:, 2:4]  \n",
    "\n",
    "       # extract image index and image class\n",
    "        imageIndex, imageClass = scaledTargets[:, :2].long().T \n",
    "\n",
    "        # gridX, gridY, gridW, gridH respectively represent the x, y, w, h on the grid\n",
    "        # extract grid x,y values \n",
    "        gridXY = scaledTargets[:, 2:4]  \n",
    "        # extract grid w,h values \n",
    "        gridWH = scaledTargets[:, 4:6]  \n",
    "\n",
    "        # extract grid i,j values (grid x,y indices)\n",
    "        # gridI, gridJ represent the integer part of x, y (which grid on the current feature map) - coords of upper left corner on feature map\n",
    "        gridIJ = (gridXY - offsets).long()\n",
    "        gridI, gridJ = gridIJ.T  \n",
    "\n",
    "        # append values accordingly to corresponding lists \n",
    "        targetIndices.append((imageIndex, layerAnchorIndices, gridJ.clamp_(0, gain[3] - 1), gridI.clamp_(0, gain[2] - 1)))\n",
    "        targetBoxes.append(torch.cat((gridXY - gridIJ, gridWH), 1))  \n",
    "        targetAnchors.append(anchors[layerAnchorIndices]) \n",
    "        targetClasses.append(imageClass)  \n",
    "\n",
    "    return targetClasses, targetBoxes, targetIndices, targetAnchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def NMS(prediction, conf_thres = 0.1, iou_thres = 0.6, multi_label = True, classes = None, agnostic = False):\n",
    "    \"\"\"\n",
    "    Performs  Non-Maximum Suppression on inference results\n",
    "    Returns detections with shape:\n",
    "        nx6 (x1, y1, x2, y2, conf, cls)\n",
    "    \"\"\"\n",
    "\n",
    "    # init minimum and maximum width and height \n",
    "    minBoxWH, maxBoxWH = 2, 4096  \n",
    "    # extract number fo classes \n",
    "    numClasses = prediction[0].shape[1] - 5\n",
    "    # multiple labels per box \n",
    "    multi_label &= numClasses > 1\n",
    "    # init output list \n",
    "    output = [None] * prediction.shape[0]\n",
    "\n",
    "    # iterate through images in prediction\n",
    "    for imageIndex, imageInference in enumerate(prediction):  \n",
    "\n",
    "        # apply confidence thresholding constraints and filter out the images that have a confidence score below our min threshold value\n",
    "        imageInference = imageInference[imageInference[:, 4] > conf_thres]  \n",
    "        # apply widht-height thresholding constraints and filter out the images that do not fall within the range min-max\n",
    "        imageInference = imageInference[((imageInference[:, 2:4] > minBoxWH) & (imageInference[:, 2:4] < maxBoxWH)).all(1)]  # width-height\n",
    "\n",
    "        # check if there are no detections remaining after filtering\n",
    "        if not imageInference.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # calculate confidence score by multiplying object confidence and class confidence together \n",
    "        imageInference[..., 5:] *= imageInference[..., 4:5]  \n",
    "\n",
    "        # the bounding box attributes we have now are described by the center coordinates, as well as the height and width of the bounding box\n",
    "        # however it is easier to calculate IoU of two boxes, using coordinates of a pair of diagnal corners for each box. \n",
    "        # so we want to  transform the (center x, center y, height, width) attributes of our boxes, to (top-left corner x, top-left corner y,  right-bottom corner x, right-bottom corner y) aka (x1,y1,x2,y2)\n",
    "        box = xywh2xyxy(imageInference[:, :4])\n",
    "\n",
    "        # create an Nx6 detection matrix (xyxy, conf, cls)\n",
    "        nmsIndices, j = (imageInference[:, 5:] > conf_thres).nonzero().t()\n",
    "        imageInference = torch.cat((box[nmsIndices], imageInference[nmsIndices, j + 5].unsqueeze(1), j.float().unsqueeze(1)), 1)\n",
    "\n",
    "        # check if classes is not none \n",
    "        if classes:\n",
    "            # filter by classes\n",
    "            imageInference = imageInference[(j.view(-1, 1) == torch.tensor(classes, device = j.device)).any(1)]\n",
    "\n",
    "        # extract number of boxes\n",
    "        numBoxes = imageInference.shape[0]  \n",
    "\n",
    "        # check if there are no detections remaining after filtering\n",
    "        if not numBoxes:\n",
    "            continue\n",
    "\n",
    "        # Batched NMS\n",
    "        \n",
    "        # extract number of classes \n",
    "        c = imageInference[:, 5] * 0 if agnostic else imageInference[:, 5]  \n",
    "        # extract boxes offset by class and scores\n",
    "        boxes, scores = imageInference[:, :4].clone() + c.view(-1, 1) * maxBoxWH, imageInference[:, 4]  \n",
    "        # preform nms and store indices of elements to keep\n",
    "        nmsIndices = torchvision.ops.boxes.nms(boxes, scores, iou_thres)\n",
    "\n",
    "        # preform merge NMS using weighted mean \n",
    "        if (1 < numBoxes < 3E3):  \n",
    "            try:  \n",
    "                # create iou matrix \n",
    "                iou = boxIOU(boxes[nmsIndices], boxes) > iou_thres  # iou matrix\n",
    "                # calculate box weights \n",
    "                weights = iou * scores[None]  \n",
    "                # merge boxes \n",
    "                imageInference[nmsIndices, :4] = torch.mm(weights, imageInference[:, :4]).float() / weights.sum(1, keepdim = True)  \n",
    "            except: \n",
    "                print(imageInference, nmsIndices, imageInference.shape, nmsIndices.shape)\n",
    "                pass\n",
    "\n",
    "        output[imageIndex] = imageInference[nmsIndices]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToTarget(output, width, height):\n",
    "    \"\"\"\n",
    "    Convert a YOLO model output to target format\n",
    "    [batch_id, class_id, x, y, w, h, conf]\n",
    "    \"\"\"\n",
    "    # check if output is a PyTorch tensor and convert to numpy array \n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.cpu().numpy()\n",
    "    \n",
    "    # init targets list \n",
    "    targets = []\n",
    "    \n",
    "    # iterate through outputs \n",
    "    for index, currOutput in enumerate(output):\n",
    "        # check if current output is not empty \n",
    "        if currOutput is not None:\n",
    "            # iterate through predictions in current output \n",
    "            for prediction in currOutput:\n",
    "                # extract bounding box for current prediction\n",
    "                box = prediction[:4]\n",
    "                # extract width of bounding box \n",
    "                widthBox = (box[2] - box[0]) / width\n",
    "                # extract height of bounding box \n",
    "                heightBox = (box[3] - box[1]) / height\n",
    "                # extract x coordinate of bounding box\n",
    "                xBox = box[0] / width + widthBox / 2\n",
    "                # extract y coordinate of bounding box \n",
    "                yBox = box[1] / height + heightBox / 2\n",
    "                # extract confidence score \n",
    "                conf = prediction[4]\n",
    "                # extract box's predicted class \n",
    "                classID = int(prediction[5])\n",
    "                # append to targets \n",
    "                targets.append([index, classID, xBox, yBox, widthBox, heightBox, conf])\n",
    "\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotImages(images, targets, paths = None, fname ='images.jpg', names = None, max_size = 640, max_subplots = 16):\n",
    "    \n",
    "    # init line thickness \n",
    "    lineThickness = 3  \n",
    "    #  init font thickness\n",
    "    fontThickness = max(lineThickness - 1, 1)  \n",
    "    \n",
    "    # check if file arealdy exists and do not overrwrite \n",
    "    if os.path.isfile(fname):  \n",
    "        return None\n",
    "    # check if images are a PyTorch tensor and convert to numpy\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.cpu().numpy()\n",
    "    # check if targets are a PyTorch tensor and convert to numpy \n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.cpu().numpy()\n",
    "\n",
    "    # un-normalise images \n",
    "    if np.max(images[0]) <= 1:\n",
    "        images *= 255\n",
    "\n",
    "    # extract batchSize, height, width from image shape \n",
    "    batchSize, _, height, width = images.shape\n",
    "    # calculate batch size as min of batch size and the max number of subplots   \n",
    "    batchSize = min(batchSize, max_subplots)\n",
    "    # calculate number of square subplots \n",
    "    numSubPlots = np.ceil(batchSize ** 0.5)  \n",
    "\n",
    "    # calculate scale factor \n",
    "    scaleFactor = max_size / max(height, width)\n",
    "    # check if resizing is necessary \n",
    "    if scaleFactor < 1:\n",
    "        height = math.ceil(scaleFactor * height)\n",
    "        width = math.ceil(scaleFactor * width)\n",
    "\n",
    "    # init empty array for output\n",
    "    mosaic = np.full((int(numSubPlots * height), int(numSubPlots * width), 3), 255, dtype = np.uint8)\n",
    "\n",
    "    # craete class - colour lookup table \n",
    "    propertyCycle = plt.rcParams['axes.prop_cycle']\n",
    "    hex2rgb = lambda height: tuple(int(height[1 + index:1 + index + 2], 16) for index in (0, 2, 4))\n",
    "    colourLookUpTable = [hex2rgb(height) for height in propertyCycle.by_key()['color']]\n",
    "\n",
    "    # iterate through images\n",
    "    for index, img in enumerate(images):\n",
    "        \n",
    "        # check if we have reached max number of subplots \n",
    "        if index == max_subplots:  \n",
    "            break\n",
    "        \n",
    "        # calculate block x value\n",
    "        block_x = int(width * (index // numSubPlots))\n",
    "        # calculate block y value \n",
    "        block_y = int(height * (index % numSubPlots))\n",
    "        # transpose image accordingly \n",
    "        img = img.transpose(1, 2, 0)\n",
    "        \n",
    "        # check if image needs to be resized \n",
    "        if scaleFactor < 1:\n",
    "            img = cv2.resize(img, (width, height))\n",
    "\n",
    "        # assign image to mosaic \n",
    "        mosaic[block_y:block_y + height, block_x:block_x + width, :] = img\n",
    "        \n",
    "        # calculate number of targets \n",
    "        numTargets = len(targets) \n",
    "\n",
    "        # check if number of targets is larger than zero \n",
    "        if numTargets > 0:\n",
    "            # extract image targets\n",
    "            image_targets = targets[targets[:, 0] == index]\n",
    "            # extract bounding boxes \n",
    "            boxes = xywh2xyxy(image_targets[:, 2:6]).T\n",
    "            # extract classes \n",
    "            classes = image_targets[:, 1].astype('int')\n",
    "            # ground truth if no confidence column\n",
    "            groundTruth = image_targets.shape[1] == 6\n",
    "            # check for confidence precense \n",
    "            conf = None if groundTruth else image_targets[:, 6]  \n",
    "\n",
    "            boxes[[0, 2]] *= width\n",
    "            boxes[[0, 2]] += block_x\n",
    "            boxes[[1, 3]] *= height\n",
    "            boxes[[1, 3]] += block_y\n",
    "\n",
    "            # iterate through boxes \n",
    "            for j, box in enumerate(boxes.T):\n",
    "                # extract image class \n",
    "                imgCls = int(classes[j])\n",
    "                imgCls = names[imgCls] if names else imgCls\n",
    "                # extract colour from look-up table  \n",
    "                color = colourLookUpTable[imgCls % len(colourLookUpTable)]\n",
    "                \n",
    "                # confidence threshold \n",
    "                if groundTruth or conf[j] > 0.3:\n",
    "                    # extract label and plot box \n",
    "                    label = '%s' % imgCls if groundTruth else '%s %.1f' % (imgCls, conf[j])\n",
    "                    plotBox(box, mosaic, label = label, color = color, line_thickness = lineThickness)\n",
    "\n",
    "        # check if paths is not none and draw image filename labels\n",
    "        if paths is not None:\n",
    "            # trim label to fourty characters\n",
    "            label = os.path.basename(paths[index])[:40] \n",
    "            # get text size \n",
    "            textSize = cv2.getTextSize(label, 0, fontScale = lineThickness / 3, thickness = fontThickness)[0]\n",
    "            # add text to image \n",
    "            cv2.putText(mosaic, label, (block_x + 5, block_y + textSize[1] + 5), 0, lineThickness / 3, [220, 220, 220], thickness = fontThickness, lineType = cv2.LINE_AA)\n",
    "\n",
    "        # create image border\n",
    "        cv2.rectangle(mosaic, (block_x, block_y), (block_x + width, block_y + height), (255, 255, 255), thickness = 3)\n",
    "\n",
    "    # resize mosaic accordingly \n",
    "    mosaic = cv2.resize(mosaic, (int(numSubPlots * width * 0.5), int(numSubPlots * height * 0.5)), interpolation = cv2.INTER_AREA)\n",
    "    # save mosaic  \n",
    "    cv2.imwrite(fname, cv2.cvtColor(mosaic, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots one bounding box on image img\n",
    "def plotBox(x, img, color = None, label = None, line_thickness = None):\n",
    "    \n",
    "    # init line thickness\n",
    "    lineThickness = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1\n",
    "    # start point and sned point for rectangle \n",
    "    startPoint, endPoint = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    # draw rectangle on image \n",
    "    cv2.rectangle(img, startPoint, endPoint, color, thickness = lineThickness, lineType = cv2.LINE_AA)\n",
    "\n",
    "    # check if label is not none \n",
    "    if label:\n",
    "        # calculate font thickness \n",
    "        fontThickness = max(lineThickness - 1, 1)  \n",
    "        # calculate text size \n",
    "        textSize = cv2.getTextSize(label, 0, fontScale = lineThickness / 3, thickness = fontThickness)[0]\n",
    "        # cecalculate end point\n",
    "        endPoint = startPoint[0] + textSize[0], startPoint[1] - textSize[1] - 3\n",
    "        # draw rectangle for label and fill it \n",
    "        cv2.rectangle(img, startPoint, endPoint, color, -1, cv2.LINE_AA)  \n",
    "        # place text in rectangle \n",
    "        cv2.putText(img, label, (startPoint[0], startPoint[1] - 2), 0, lineThickness / 3, [225, 255, 255], thickness = fontThickness, lineType = cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(start = 0, stop = 0, bucket ='', id =()):  \n",
    "\n",
    "    # create list of graph titles \n",
    "    graphTitles = ['GIoU', 'Objectness', 'Classification', 'Precision', 'Recall', 'val GIoU', 'val Objectness', 'val Classification', 'mAP@0.5', 'F1']\n",
    "    # create figure, axis instance \n",
    "    figure, axis = plt.subplots(2, 5, figsize =(12, 6), tight_layout = True)\n",
    "    axis = axis.ravel()\n",
    "    # extract files\n",
    "    files = glob.glob('results*.txt') + glob.glob('../../Downloads/results*.txt')\n",
    "    \n",
    "    # iterate through files\n",
    "    for file in sorted(files):\n",
    "        # load text from file and assign to results\n",
    "        results = np.loadtxt(file, usecols =[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin = 2).T\n",
    "        # extract number of rows\n",
    "        numRows = results.shape[1] \n",
    "        x = numRows\n",
    "\n",
    "        for i in range(10):\n",
    "            y = results[i, x]\n",
    "\n",
    "            # do not show loss values of zero \n",
    "            if i in [0, 1, 2, 5, 6, 7]:\n",
    "                y[y == 0] = np.nan\n",
    "            \n",
    "            # plot and set title \n",
    "            axis[i].plot(x, y, marker ='.', label = Path(file).stem, linewidth = 2, markersize = 8)\n",
    "            axis[i].set_title(graphTitles[i])\n",
    "\n",
    "    # show legend \n",
    "    axis[1].legend()\n",
    "    # save figure as png\n",
    "    figure.savefig('results.png', dpi = 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
