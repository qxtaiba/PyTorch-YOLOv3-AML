{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --user # !pip3 install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data\n",
    "\n",
    "!bash get_coco2017.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ..\n",
    "\n",
    "!cd weights\n",
    "\n",
    "!wget https://pjreddie.com/media/files/yolov3.weights\n",
    "    \n",
    "!cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ExifTags\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from sys import platform\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptedImageFormats = ['.bmp', '.jpg', '.jpeg', '.png', '.tif', '.tiff', '.dng']\n",
    "\n",
    "# find the orientation of the exif tag\n",
    "for orientation in ExifTags.TAGS.keys():\n",
    "    if ExifTags.TAGS[orientation] == 'Orientation':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images - Inference Dataset Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImages: \n",
    "    def __init__(self, path, imageSize = 416):\n",
    "        \n",
    "        # init files list  \n",
    "        files = []\n",
    "        # extract path \n",
    "        path = str(Path(path))  \n",
    "        \n",
    "        # check if path leads to a directory  and populate files list\n",
    "        if os.path.isdir(path):\n",
    "            files = sorted(glob.glob(os.path.join(path, '*.*')))\n",
    "        # check if path leads to a file and populate files list \n",
    "        elif os.path.isfile(path):\n",
    "            files = [path]\n",
    "\n",
    "        # extract image(s) if they are in the correct format \n",
    "        images = [x for x in files if os.path.splitext(x)[-1].lower() in acceptedImageFormats]\n",
    "        # extract number of images\n",
    "        numImages = len(images)\n",
    "        # init image size \n",
    "        self.imgSize = imageSize\n",
    "        # init files \n",
    "        self.files = images \n",
    "        # init number of files \n",
    "        self.numFiles = numImages \n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        # init count to zero \n",
    "        self.count = 0\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "\n",
    "        # check if we have loaded all of the images \n",
    "        if self.count == self.numFiles:\n",
    "            raise StopIteration\n",
    "        \n",
    "        # extract path\n",
    "        path = self.files[self.count]\n",
    "        # increment count\n",
    "        self.count += 1\n",
    "        # read image\n",
    "        img0 = cv2.imread(path)\n",
    "        # resize by adding padding\n",
    "        img = letterbox(img0, newShape = self.imgSize)[0]\n",
    "        # convert image from BGR to RGB and to 3x416x416\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return path, img, img0\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # return number of files\n",
    "        return self.numFiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images and Labels - Training and Testing Dataset Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get EXIF Size - Dataset Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# return the exif-corrected PIL size\n",
    "def getEXIFsize(img):\n",
    "\n",
    "    #extract image size\n",
    "    shape = img.size\n",
    "    try:        \n",
    "        # rotation by 270\n",
    "        if dict(img._getexif().items())[orientation] == 6:\n",
    "            shape = (shape[1], shape[0])\n",
    "        # rotation by 90\n",
    "        elif dict(img._getexif().items())[orientation] == 8:\n",
    "            shape = (shape[1], shape[0])    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image - Dataset Helper: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the following function is to load a single image from the COCO dataset. The function works by extracting the image path and reading it using the cv2.imread() method. The cv2.imread() method loads an image from the specified file. If the image cannot be read (because of missing file, improper permissions, unsupported or invalid format) then this method returns an empty matrix. The height and width of the image are then extracted and we calculate the resize factor such that we can resize the image to a preset image size. The resizing of the image is done using the cv2.resize() method with either bilinear interpolation, or alternatively a variation of a nearest-neighbour interpolation that resamples using the pixel area resolution. The function then returns the image, its height and width, as well as its resized height and width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(self, index):\n",
    "\n",
    "    # extract path \n",
    "    path = self.imgFiles[index]\n",
    "    # read image \n",
    "    img = cv2.imread(path)  \n",
    "    # extract height and width of image \n",
    "    originalHeight, originalWidth = img.shape[:2]  \n",
    "    # resize factor so that we can resize image to imageSize\n",
    "    resizeFactor = self.imageSize / max(originalHeight, originalWidth)\n",
    "    \n",
    "    # always resize down, only resize up if training with augmentation\n",
    "    if resizeFactor != 1:\n",
    "        # interpolate image\n",
    "        interp = cv2.INTER_AREA if resizeFactor < 1 and not self.isAugment else cv2.INTER_LINEAR\n",
    "        # resize image\n",
    "        img = cv2.resize(img, (int(originalWidth * resizeFactor), int(originalHeight * resizeFactor)), interpolation = interp)\n",
    "    \n",
    "    # extract height and width of resized image \n",
    "    resizedHeight, resizedWidth = img.shape[:2]\n",
    "\n",
    "    return img, (originalHeight, originalWidth), (resizedHeight, resizedWidth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment HSV - Dataset Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to modify an input image in-place by manipulating its hue, saturation, and value. It is important to note that hue, saturation, and value are the main colour properties that allow us to distinguish between different colours. Modifying these values allows us to augment our input image, expand out dataset, and improve our training results.\n",
    "\n",
    "\n",
    "1. Hues are the three primary colours (red, blue, and yellow) and the three secondary colours (orange, green, and violet) that appear in the colour wheel or colour circle. When you refer to hue, you are referring to pure colour, or the visible spectrum of basic colours that can be seen in a rainbow. \n",
    "\n",
    "2. Colour saturation is the purity and intensity of a colour as displayed in an image. The higher the saturation of a colour, the more vivid and intense it is. The lower a colour’s saturation, or chroma, the closer it is to pure grey on the grayscale.\n",
    "\n",
    "3. Colour value refers to the relative lightness or darkness of a colour. We perceive colour value based on the quantity of light reflected off of a surface and absorbed by the human eye. We refer to the intensity of the light that reaches the eye as “luminance.”\n",
    "\n",
    "\n",
    "The cv2.LUT() method allows us to create a lookup-table with randomly generated values that are used to  transform the image’s hue, saturation, and value to new values. The image is then modified using cv2.cvtColor() to to convert the image from one colour space to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentHSV(img, hgain = 0.5, sgain = 0.5, vgain = 0.5):\n",
    "    \n",
    "    # init random gains\n",
    "    randomGains = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  \n",
    "    # extract hue, saturation, value from image\n",
    "    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
    "    # init numpy array\n",
    "    x = np.arange(0, 256, dtype = np.int16)\n",
    "    # init look-up table for hue with random gain\n",
    "    lookUpHue = ((x * randomGains[0]) % 180).astype(img.dtype)\n",
    "    # init look-up table for saturation with random gain \n",
    "    lookUpSat = np.clip(x * randomGains[1], 0, 255).astype(img.dtype)\n",
    "    # init look-up table for value with random gain\n",
    "    lookUpVal = np.clip(x * randomGains[2], 0, 255).astype(img.dtype)\n",
    "    # extract new hue, saturation, value for image using look-up tables\n",
    "    modifiedHSV = cv2.merge((cv2.LUT(hue, lookUpHue), cv2.LUT(sat, lookUpSat), cv2.LUT(val, lookUpVal))).astype(img.dtype)\n",
    "    # modify image\n",
    "    cv2.cvtColor(modifiedHSV, cv2.COLOR_HSV2BGR, dst = img)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mosaic - Dataset Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to load images into a mosaic of four. It is a form of augmentation that is used only during training and it works by taking a total of four images, creating a base image with the corresponding number of tiles, and then calculating the position of each image on the base image. It also calculates the required padding, normalises the image labels, and then concatenates/clips the labels and applies an augmentation to both the images and labels, and returns them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic(self, index):   \n",
    "\n",
    "    # init labels \n",
    "    mosaicLabels = []\n",
    "    # extract image size\n",
    "    imageSize = self.imageSize\n",
    "    # randomly init center coordinates\n",
    "    centerX, centerY = [int(random.uniform(imageSize * 0.5, imageSize * 1.5)) for _ in range(2)]\n",
    "    # randomly init an additional three image indices\n",
    "    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]\n",
    "    \n",
    "    for i, imageIndex in enumerate(indices):\n",
    "        # load current image\n",
    "        img, (originalHeight, originalWidth), (resizedHeight, resizedWidth) = loadImage(self, imageIndex)\n",
    "\n",
    "        if i == 0: # top left\n",
    "            # create base image with 4 tiles\n",
    "            baseImage = np.full((imageSize * 2, imageSize * 2, img.shape[2]), 114, dtype = np.uint8)  \n",
    "            # xmin, ymin, xmax, ymax for large image\n",
    "            xMinlarge, yminLarge, xMaxLarge, yMaxLarge = max(centerX - resizedWidth, 0), max(centerY - resizedHeight, 0), centerX, centerY\n",
    "            # xmin, ymin, xmax, ymax for small image\n",
    "            xMinSmall, yMinSmall, xMaxSmall, yMaxSmall = resizedWidth - (xMaxLarge - xMinlarge), resizedHeight - (yMaxLarge - yminLarge), resizedWidth, resizedHeight  \n",
    "        \n",
    "        elif i == 1:  # top right\n",
    "            # xmin, ymin, xmax, ymax for large image\n",
    "            xMinlarge, yminLarge, xMaxLarge, yMaxLarge = centerX, max(centerY - resizedHeight, 0), min(centerX + resizedWidth, imageSize * 2), centerY\n",
    "            # xmin, ymin, xmax, ymax for small image\n",
    "            xMinSmall, yMinSmall, xMaxSmall, yMaxSmall = 0, resizedHeight - (yMaxLarge - yminLarge), min(resizedWidth, xMaxLarge - xMinlarge), resizedHeight\n",
    "        \n",
    "        elif i == 2: # bottom left\n",
    "            # xmin, ymin, xmax, ymax for large image\n",
    "            xMinlarge, yminLarge, xMaxLarge, yMaxLarge = max(centerX - resizedWidth, 0), centerY, centerX, min(imageSize * 2, centerY + resizedHeight)\n",
    "            # xmin, ymin, xmax, ymax for small image\n",
    "            xMinSmall, yMinSmall, xMaxSmall, yMaxSmall = resizedWidth - (xMaxLarge - xMinlarge), 0, max(centerX, resizedWidth), min(yMaxLarge - yminLarge, resizedHeight)\n",
    "        \n",
    "        elif i == 3: # bottom right\n",
    "            # xmin, ymin, xmax, ymax for large image\n",
    "            xMinlarge, yminLarge, xMaxLarge, yMaxLarge = centerX, centerY, min(centerX + resizedWidth, imageSize * 2), min(imageSize * 2, centerY + resizedHeight)\n",
    "            # xmin, ymin, xmax, ymax for small image\n",
    "            xMinSmall, yMinSmall, xMaxSmall, yMaxSmall = 0, 0, min(resizedWidth, xMaxLarge - xMinlarge), min(yMaxLarge - yminLarge, resizedHeight)\n",
    "\n",
    "        # init base image parameters \n",
    "        baseImage[yminLarge:yMaxLarge, xMinlarge:xMaxLarge] = img[yMinSmall:yMaxSmall, xMinSmall:xMaxSmall]  \n",
    "        \n",
    "        # calculate padding \n",
    "        widthPadding = xMinlarge - xMinSmall\n",
    "        heightPadding = yminLarge - yMinSmall\n",
    "\n",
    "        # extract labels\n",
    "        labels = self.labels[imageIndex]\n",
    "        _labels = labels.copy()\n",
    "\n",
    "        # normalize xywh to xyxy format\n",
    "        if labels.size > 0:\n",
    "            _labels[:, 1] = resizedWidth * (labels[:, 1] - labels[:, 3] / 2) + widthPadding\n",
    "            _labels[:, 2] = resizedHeight * (labels[:, 2] - labels[:, 4] / 2) + heightPadding\n",
    "            _labels[:, 3] = resizedWidth * (labels[:, 1] + labels[:, 3] / 2) + widthPadding\n",
    "            _labels[:, 4] = resizedHeight * (labels[:, 2] + labels[:, 4] / 2) + heightPadding\n",
    "        \n",
    "        mosaicLabels.append(_labels)\n",
    "\n",
    "    # check if mosaicLabels is not empty\n",
    "    if len(mosaicLabels):\n",
    "        # concatenate labels\n",
    "        mosaicLabels = np.concatenate(mosaicLabels, 0)\n",
    "        # clip labels\n",
    "        np.clip(mosaicLabels[:, 1:], 0, 2 * imageSize, out = mosaicLabels[:, 1:])\n",
    "\n",
    "    # augment images and labels\n",
    "    baseImage, mosaicLabels = randAffine(baseImage, mosaicLabels,degrees = self.hyp['degrees'], translate = self.hyp['translate'], scale = self.hyp['scale'], shear = self.hyp['shear'], border = -imageSize // 2)  # border to remove\n",
    "\n",
    "    return baseImage, mosaicLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letterbox - Dataset Helper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to resize an input image into a 32-pixel-multiple rectangle. This reduces  the inference time proportionally to the amount of letterboxed area padded onto a square image. It works by extracting the current shape, calculating the necessary padding, resizing it if necessary, and then creating and adding a border using the cv2.copyMakeBorder() method. It returns the letterboxed image, the scaling ratio, as well as the padding used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, newShape = (416, 416), color = (114, 114, 114), auto = True, scaleFill = False, scaleup = True):\n",
    "\n",
    "    # extract current image shape\n",
    "    currShape = img.shape[:2]\n",
    "\n",
    "    # check if new image shape is an integer or a tuple\n",
    "    if isinstance(newShape, int):\n",
    "        # create tuple\n",
    "        newShape = (newShape, newShape)\n",
    "    \n",
    "    # calculate scale ratio by dividing new shape by old shape\n",
    "    scaleRatio = min(newShape[0] / currShape[0], newShape[1] / currShape[1])\n",
    "\n",
    "    # only scale down, do not scale up\n",
    "    if not scaleup:\n",
    "        scaleRatio = min(scaleRatio, 1.0)\n",
    "\n",
    "    # extract unpadded shape\n",
    "    unpaddedShape = (int(round(currShape[1] * scaleRatio)), int(round(currShape[0] * scaleRatio)))\n",
    "    # calculate width and height padding \n",
    "    widthPadding, heightPadding = newShape[1] - unpaddedShape[0], newShape[0] - unpaddedShape[1]  \n",
    "    \n",
    "    if auto:  \n",
    "        widthPadding, heightPadding = np.mod(widthPadding, 32), np.mod(heightPadding, 32)  # wh padding\n",
    "\n",
    "    # resize image if current shape does not equal the unpadded shape \n",
    "    if currShape[::-1] != unpaddedShape:\n",
    "        img = cv2.resize(img, unpaddedShape, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    # divide width padding into two sides (left/right)\n",
    "    widthPadding /= 2\n",
    "    # divide height padding into two side (bottom/above)\n",
    "    heightPadding /= 2\n",
    "    # create top/bottom border\n",
    "    top, bottom = int(round(heightPadding - 0.1)), int(round(heightPadding + 0.1))\n",
    "    # create left/right border \n",
    "    left, right = int(round(widthPadding - 0.1)), int(round(widthPadding + 0.1))\n",
    "    # add borders\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value = color)  \n",
    "\n",
    "    return img, (scaleRatio, scaleRatio), (widthPadding, heightPadding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Affine - Dataset Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is another form of dataset augmentation used to apply rotate, scale, translate, and shear transforms to an input image. It also transforms the label coordinates and returns the image and label coordinates. \n",
    "\n",
    "It works by creating and initialising a rotate/scale matrix, a translate matrix, and a shear matrix. These matrices are then combined to create a combined rotation matrix. \n",
    "\n",
    "The function then checks if the image needs to be changed and applies the cv2.warpAffine method with linear interpolation. This function applies an affine transformation to the image. It then transforms the label coordinates by first warping the coordinates, creating new boxes and filtering out warped points outside of the image bounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randAffine(img, targets =(), degrees = 10, translate =.1, scale =.1, shear = 10, border = 0):\n",
    "\n",
    "    constVal = 1e-16\n",
    "    \n",
    "    # calculate height\n",
    "    height = img.shape[0] + border * 2\n",
    "    # calculate width \n",
    "    width = img.shape[1] + border * 2\n",
    "\n",
    "    # rotate and scale by first creating 3x3 identity matrix \n",
    "    R = np.eye(3)\n",
    "    # init random angle value \n",
    "    angle = random.uniform(-degrees, degrees)\n",
    "    # init random scale valye \n",
    "    scale = random.uniform(1 - scale, 1 + scale)\n",
    "    # create rotation matrix\n",
    "    R[:2] = cv2.getRotationMatrix2D(angle = angle, center =(img.shape[1] / 2, img.shape[0] / 2), scale = scale)\n",
    "\n",
    "    # translate by first creating 3x3 identity matrix\n",
    "    T = np.eye(3)\n",
    "    # init x translation\n",
    "    T[0, 2] = random.uniform(-translate, translate) * img.shape[0] + border  \n",
    "    # init y translation\n",
    "    T[1, 2] = random.uniform(-translate, translate) * img.shape[1] + border  \n",
    "\n",
    "    # shear by first creating 3x3 identity matrix\n",
    "    S = np.eye(3)\n",
    "    # init x shear [deg]\n",
    "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  \n",
    "    # init y shear [deg] \n",
    "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  \n",
    "\n",
    "    # creatte combined rotation matrix\n",
    "    M = S @ T @ R\n",
    "\n",
    "    # check if image changed\n",
    "    if (border != 0) or (M != np.eye(3)).any():\n",
    "        img = cv2.warpAffine(img, M[:2], dsize = (width, height), flags = cv2.INTER_LINEAR, borderValue = (114, 114, 114))\n",
    "\n",
    "    # transform label coordinates\n",
    "    if len(targets):\n",
    "\n",
    "        # warp points\n",
    "        xy = np.ones((len(targets) * 4, 3))\n",
    "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(len(targets) * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "        xy = (xy @ M.T)[:, :2].reshape(len(targets), 8)\n",
    "\n",
    "        # create new boxes\n",
    "        x = xy[:, [0, 2, 4, 6]]\n",
    "        y = xy[:, [1, 3, 5, 7]]\n",
    "        xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, len(targets)).T\n",
    "\n",
    "        # reject warped points outside of image\n",
    "        xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n",
    "        xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n",
    "        \n",
    "        # extract width\n",
    "        w = xy[:, 2] - xy[:, 0]\n",
    "        # extract height\n",
    "        h = xy[:, 3] - xy[:, 1]\n",
    "\n",
    "        # calculate area \n",
    "        area = w * h\n",
    "        area0 = (targets[:, 3] - targets[:, 1]) * (targets[:, 4] - targets[:, 2])\n",
    "        \n",
    "        # calculate aspect ratio\n",
    "        aspectRatio = np.maximum(w/(h + constVal), h/(w + constVal))  \n",
    "        \n",
    "        i = (w > 4) & (h > 4) & (area / (area0 * scale + constVal) > 0.2) & (aspectRatio < 10)\n",
    "\n",
    "        targets = targets[i]\n",
    "        targets[:, 1:5] = xy[i]\n",
    "\n",
    "    return img, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set printoptions\n",
    "torch.set_printoptions(linewidth = 320, precision = 5, profile ='long')\n",
    "np.set_printoptions(linewidth = 320, formatter ={'float_kind': '{:11.5g}'.format})  # format short g, %precision = 5\n",
    "matplotlib.rc('font', **{'size': 11})\n",
    "\n",
    "# prevent OpenCV from multithreading in order to use PyTorch DataLoader\n",
    "cv2.setNumThreads(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XYXYtoXYWH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function converts boxes from  (x1, y1, x2, y2) to (x, y, w, h) where x1, y1 represent the top-left coordinates and x2, y2 represent the bottom-right coordinates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert boxes from [x1, y1, x2, y2] to [x, y, w, h] where x1, y1 represents the top-left coordinates and x2, y2 represents the bottom-right\n",
    "def xyxy2xywh(x):\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # center x coordinate\n",
    "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # center y coordinate\n",
    "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
    "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XYWHtoXYXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, similarly to the above, converts boxes from (x, y, w, h) to (x1, y1, x2, y2) where x1, y1 represent the top-left coordinates and x2, y2 represent the bottom-right coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1 = top-left, xy2 = bottom-right\n",
    "def xywh2xyxy(x):\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top-left x coordinate\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top-left y coordinate\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom-right x coordinate\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom-right y coordinate\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to rescale the coordinates extracted from the shape of img1 to that of the shape of img0. It works by calculating the gain and width/height padding required and then rescaling the coordinates and clipping the bounding boxes to the image’s shape using the torch.clamp() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale coordinates from first shape to that of second shape \n",
    "def scaleCoordinates(img1_shape, coords, img0_shape, ratio_pad = None):\n",
    "    \n",
    "    # check if ratioPad is set to None\n",
    "    if ratio_pad is None:  \n",
    "        # calculate gain (old/new) from img0_shape\n",
    "        gain = max(img1_shape) / max(img0_shape) \n",
    "        # calculate width and height padding from img0_shape\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2 \n",
    "    \n",
    "    else:\n",
    "        # calculate gain from ratioPad\n",
    "        gain = ratio_pad[0][0]\n",
    "        # calculate padding from ratioPad \n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    # extract x and y padding valyes \n",
    "    xPadding, yPadding = pad[0], pad[1]\n",
    "\n",
    "    # rescale coordinates\n",
    "    coords[:, [0, 2]] -= xPadding \n",
    "    coords[:, [1, 3]] -= yPadding  \n",
    "    coords[:, :4] /= gain\n",
    "\n",
    "    # clip bounding xyxy bounding boxes to image shape (height, width)\n",
    "    coords[:, 0].clamp_(0, img0_shape[1])  # x1\n",
    "    coords[:, 1].clamp_(0, img0_shape[0])  # y1\n",
    "    coords[:, 2].clamp_(0, img0_shape[1])  # x2\n",
    "    coords[:, 3].clamp_(0, img0_shape[0])  # y2\n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleImage(img, ratio = 1.0, same_shape = True):  # img(16,3,256,416), r = ratio\n",
    "    \n",
    "    # extract width and height \n",
    "    height, width = img.shape[2:]\n",
    "    # calculate new size \n",
    "    newSize = (int(height * ratio), int(width * ratio))  \n",
    "    # resize image\n",
    "    img = F.interpolate(img, size = newSize, mode ='bilinear', align_corners = False)  \n",
    "    \n",
    "    # check if isSameShape set to false \n",
    "    if not same_shape:\n",
    "        # calculate height and wdith to be used for padding/cropping image\n",
    "        gridSize = 64  \n",
    "        height, width = [math.ceil(x * ratio / gridSize) * gridSize for x in (height, width)]\n",
    "        \n",
    "    # pad image \n",
    "    return F.pad(img, [0, width - newSize[1], 0, height - newSize[0]], value = 0.447)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precion Per Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the average precision using recall and precision curves. It takes in a numpy array of true positives, objectness values, predicted object classes, and true object classes in order to return the average precision on a class by class basis. More information about this function can be found here: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "\n",
    "It works by first sorting by objectness and extracting the sorted indices, extracting unique classes, and iterating through each class stored in unique classes and finding the number of ground truth and predicted objects. If these values are larger than 0, we accumulate the False/True positives and use them to create the recall and precision curves. These curves allow us to then calculate AP and F1, where F1 is the harmonic mean of the precision and recall values. \n",
    "\n",
    "The equations for the calculations described above are as follows:\n",
    "\n",
    "In order to calculate the average precision, please see the next block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we want to create a precision-recall curve and compute the average precision for each class\n",
    "# F1 score is (harmonic mean of precision and recall)\n",
    "def getAPClass(truePositives, objectnessVal, predictedClasses, targetClasses):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:    True positives (nparray, nx1 or nx10).\n",
    "        conf:  Objectness value from 0-1 (nparray).\n",
    "        pred_cls: Predicted object classes (nparray).\n",
    "        target_cls: True object classes (nparray).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # sort by objectness and store sorted indices in objectnessSortIndices\n",
    "    objectnessSortIndices = np.argsort(-objectnessVal)\n",
    "    # sort truePositive using sorted indices\n",
    "    truePositives = truePositives[objectnessSortIndices]\n",
    "    # sort objectnessVal using sorted indices\n",
    "    objectnessVal = objectnessVal[objectnessSortIndices]\n",
    "    # sort predClasses using sorted indices\n",
    "    predictedClasses = predictedClasses[objectnessSortIndices]\n",
    "    # find all unique classes \n",
    "    uniqueClasses = np.unique(targetClasses)\n",
    "    # init constant val\n",
    "    constVal = 1e-16\n",
    "\n",
    "    # score to evaluate P and R https://github.com/ultralytics/yolov3/issues/898\n",
    "    precisionScore = 0.1  \n",
    "    shape = [uniqueClasses.shape[0], truePositives.shape[1]]  # number class, number iou thresholds (i.e. 10 for mAP0.5...0.95)\n",
    "    AP, precision, recall = np.zeros(shape), np.zeros(shape), np.zeros(shape)\n",
    "\n",
    "    # iterate through each class stored in unique classes\n",
    "    for classIndex, uniqueClass in enumerate(uniqueClasses):\n",
    "        objectnessSortIndices = predictedClasses == uniqueClass\n",
    "        # find number of ground truth objects\n",
    "        numGroundTruthObjects = (targetClasses == uniqueClass).sum() \n",
    "        # find number of predicted objects \n",
    "        numPredictedObjects = objectnessSortIndices.sum()  \n",
    "\n",
    "        # if there are no predicted objects AND no ground truth objects then we just skip this loop \n",
    "        if numPredictedObjects == 0 or numGroundTruthObjects == 0:\n",
    "            continue\n",
    "        \n",
    "        # otherwise if both number of predicted objects and number of ground truth objects are both non-zero\n",
    "        else:\n",
    "            # find the cumulative sum of false positives \n",
    "            cumulativeFalsePositives = (1 - truePositives[objectnessSortIndices]).cumsum(0)\n",
    "            # find the cumulative sum of true positives\n",
    "            cumulativeTruePositives = truePositives[objectnessSortIndices].cumsum(0)\n",
    "\n",
    "            # create the recall curve and append it to list\n",
    "            recallCurve = cumulativeTruePositives / (numGroundTruthObjects + constVal)  \n",
    "            # calculate recall at precisionScore\n",
    "            recall[classIndex] = np.interp(-precisionScore, -objectnessVal[objectnessSortIndices], recallCurve[:, 0]) \n",
    "\n",
    "            # create the precision curve and append it to list\n",
    "            precisionCurve = cumulativeTruePositives / (cumulativeTruePositives + cumulativeFalsePositives)  \n",
    "            # calculate precision at precisionScore\n",
    "            precision[classIndex] = np.interp(-precisionScore, -objectnessVal[objectnessSortIndices], precisionCurve[:, 0]) \n",
    "\n",
    "            # calculate AP from recall-precision curve\n",
    "            for j in range(truePositives.shape[1]):\n",
    "                AP[classIndex, j] = getAP(recallCurve[:, j], precisionCurve[:, j])\n",
    "\n",
    "    # calculate F1 score\n",
    "    F1 = 2 * precision * recall / (precision + recall + constVal)\n",
    "\n",
    "    return precision, recall, AP, F1, uniqueClasses.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function works alongside the function above. It takes in the recall and precision curves in order to calculate and return the average precision. More information about this function can be found here: https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "It works by appending sentinel values to the beginning and end of the recall/precision lists, and then computing the precision envelope and integrating the area under the curve. The result of this integration is the average precision and is returned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAP(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # append sentinel values at the beginning and end of the recall curve and precision curve\n",
    "    mrec = np.concatenate(([0.], recall, [min(recall[-1] + 1E-3, 1.)]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "    # calculate the precision envelope\n",
    "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
    "    # init a 101-point interp (COCO)\n",
    "    x = np.linspace(0, 1, 101)\n",
    "    # integrate area under envelope to calculate average precision\n",
    "    AP = np.trapz(np.interp(x, mrec, mpre), x)\n",
    "\n",
    "    return AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection over Union (IOU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function returns the IoU of two boxes. It works by extracting the coordinates of the bounding boxes, calculating the coordinates of the intersection rectangles, and using that to find the intersection area. The union area is then calculated using the width and height of the two boxes and the IoU is returned. The equations used are shown below: \n",
    "\n",
    "[INSERT EQUATIONS]\n",
    "\n",
    "This function is important [TALK ABOUT WHY WE NEED THIS]. \n",
    "\n",
    "It is also important to note, however, that whilst boxIOU and widthHeightIOU  all carry out a similar role to boundingBoxIOU, boxIOU expects that the two sets of boxes are in (x1, y1, x2, y2) format. Additionally, widthHeightIOU returns an NxM IoU matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
    "def boundingBoxIOU(firstBox, secondBox, x1y1x2y2 = True, GIoU = False):\n",
    "    \n",
    "    # transpose secondBox\n",
    "    secondBox = secondBox.t()\n",
    "    # init const val \n",
    "    constVal = 1e-16\n",
    "\n",
    "    if x1y1x2y2:\n",
    "        # extract coordinates of bounding boxes - transform from center and width to exact coordinates\n",
    "        firstBoxX1, firstBoxY1 = firstBox[0], firstBox[1]\n",
    "        firstBoxX2, firstBoxY2 = firstBox[2], firstBox[3]\n",
    "        secondBoxX1, secondBoxY1 = secondBox[0], secondBox[1]\n",
    "        secondBoxX2, secondBoxY2 = secondBox[2], secondBox[3]\n",
    "\n",
    "    else:  \n",
    "        # extract coordinates of bounding boxes - transform from xywh to xyxy\n",
    "        firstBoxX1, firstBoxX2 = firstBox[0] - firstBox[2] / 2, firstBox[0] + firstBox[2] / 2\n",
    "        firstBoxY1, firstBoxY2 = firstBox[1] - firstBox[3] / 2, firstBox[1] + firstBox[3] / 2\n",
    "        secondBoxX1, secondBoxX2 = secondBox[0] - secondBox[2] / 2, secondBox[0] + secondBox[2] / 2\n",
    "        secondBoxY1, secondBoxY2 = secondBox[1] - secondBox[3] / 2, secondBox[1] + secondBox[3] / 2\n",
    "\n",
    "    # extract intersection rectangle coordinates\n",
    "    rectIntersectionX1, rectIntersectionY1  = torch.max(firstBoxX1, secondBoxX1), torch.max(firstBoxY1, secondBoxY1) \n",
    "    rectIntersectionX2, rectIntersectionY2 = torch.min(firstBoxX2, secondBoxX2), torch.min(firstBoxY2, secondBoxY2)\n",
    "    \n",
    "    # calculate intersection width\n",
    "    intersectionWidth = (rectIntersectionX2 - rectIntersectionX1).clamp(0)\n",
    "    # calculate intersection height\n",
    "    intersectionHeight = (rectIntersectionY2 - rectIntersectionY1).clamp(0)\n",
    "    # calculate intersection area \n",
    "    intersectionArea = intersectionWidth * intersectionHeight\n",
    "\n",
    "    # calculate width and height of first box \n",
    "    firstWidth, firstHeight = firstBoxX2 - firstBoxX1, firstBoxY2 - firstBoxY1\n",
    "    # calculate width and height of second box \n",
    "    secondWidth, secondHeight = secondBoxX2 - secondBoxX1, secondBoxY2 - secondBoxY1\n",
    "    # calculate union area \n",
    "    unionArea = (firstWidth * firstHeight + constVal) + secondWidth * secondHeight - intersectionArea\n",
    "\n",
    "    # calculate intersection-over-union (IoU) area\n",
    "    iou = intersectionArea / unionArea  \n",
    "    \n",
    "    # check if GIoU is true \n",
    "    if GIoU:\n",
    "        # extract smallest enclosing width (convex width)\n",
    "        smallestEnclosingWidth = torch.max(firstBoxX2, secondBoxX2) - torch.min(firstBoxX1, secondBoxX1)  \n",
    "        # extract smallest enclosing height (convex height)\n",
    "        smallestEnclosingHeight = torch.max(firstBoxY2, secondBoxY2) - torch.min(firstBoxY1, secondBoxY1) \n",
    "        # calculate smallest enclosing area (convex araea) \n",
    "        smallestEnclosingArea = smallestEnclosingWidth * smallestEnclosingHeight + constVal \n",
    "        \n",
    "        # return GIoU\n",
    "        return iou - (smallestEnclosingArea - unionArea) / smallestEnclosingArea  \n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxIOU(box1, box2):\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate width and height of first box\n",
    "    boxOneWidth = box1.t()[2] - box1.t()[0]\n",
    "    boxOneHeight = box1.t()[3] - box1.t()[1]\n",
    "    # calculate width and height of second box \n",
    "    boxTwoWdith = box2.t()[2] - box2.t()[0]\n",
    "    boxTwoHeight = box2.t()[3] - box2.t()[1]\n",
    "    # calculate area of first box\n",
    "    areaOne = boxOneWidth * boxOneHeight\n",
    "    # calculate area of second box \n",
    "    areaTwo = boxTwoWdith * boxTwoHeight\n",
    "\n",
    "    # calculate intersection area \n",
    "    intersectionArea = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    # calculate union area \n",
    "    unionArea = (areaOne[:, None] + areaTwo - intersectionArea)\n",
    "\n",
    "    return intersectionArea /  unionArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2\n",
    "def widthHeightIOU(firstWidthHeight, secondWidthHeight):\n",
    "    \n",
    "    # extract shapes \n",
    "    firstWidthHeight = firstWidthHeight[:, None]  # [N,1,2]\n",
    "    secondWidthHeight = secondWidthHeight[None]  # [1,M,2]\n",
    "    # caclulate intersection area \n",
    "    intersectionArea = torch.min(firstWidthHeight, secondWidthHeight).prod(2)  # [N,M]\n",
    "    # calculate union area \n",
    "    unionArea = (firstWidthHeight.prod(2) + secondWidthHeight.prod(2) - intersectionArea) \n",
    "\n",
    "    return intersectionArea / unionArea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLosses(predictions, targets, model):  \n",
    "    # init float tensor depending on cuda availability \n",
    "    FloatTensor = torch.cuda.FloatTensor if predictions[0].is_cuda else torch.Tensor\n",
    "\n",
    "    # init class loss tensor to zeroes\n",
    "    classLoss = FloatTensor([0])\n",
    "    # init box loss tensor to zeroes \n",
    "    GIoUBoxLoss = FloatTensor([0])\n",
    "    # init object loss tensor to zeroes \n",
    "    objectLoss = FloatTensor([0])\n",
    "\n",
    "    # calculate and extract targets \n",
    "    tcls, tbox, indices, anchors = buildTargets(predictions, targets, model)  \n",
    "\n",
    "    # define criteria for BCE loss\n",
    "    BCEcls = nn.BCEWithLogitsLoss(pos_weight = FloatTensor([model.hyp['cls_pw']]), reduction = 'mean')\n",
    "    BCEobj = nn.BCEWithLogitsLoss(pos_weight = FloatTensor([model.hyp['obj_pw']]), reduction = 'mean')\n",
    "\n",
    "    # init total number of targets to zero \n",
    "    cumNumTargets = 0  \n",
    "    \n",
    "    # iterate through each layer predection (output )\n",
    "    for layerIdx, layerPrediction in enumerate(predictions):\n",
    "        # extract image index, anchor, y grid coordinate, x grid coordinate \n",
    "        imageIndex, anchor, gridY, gridX = indices[layerIdx]  \n",
    "        # init target objectness value to tensor of zeroes \n",
    "        targetObj = torch.zeros_like(layerPrediction[..., 0])  \n",
    "        # extract number of targets \n",
    "        numTargets = imageIndex.shape[0]  \n",
    "\n",
    "        # check if number of targets is larger than zero \n",
    "        if numTargets:\n",
    "            # increment cumulative number of targets with current number of targets \n",
    "            cumNumTargets += numTargets  \n",
    "            # extract prediction subset corresponding to current targets\n",
    "            predictionSubset = layerPrediction[imageIndex, anchor, gridY, gridX]  \n",
    "\n",
    "            # extract prediction x, y coordinates \n",
    "            predictionXY = predictionSubset[:, :2].sigmoid()\n",
    "            # extract prediction w,h values \n",
    "            predictionWH = predictionSubset[:, 2:4].exp().clamp(max = 1E3) * anchors[layerIdx]\n",
    "            # create predicted boz by concatenating predictionXY and predictionWH\n",
    "            predictedBox = torch.cat((predictionXY, predictionWH), 1) \n",
    "            # calculate GIoU\n",
    "            GIoU = boundingBoxIOU(predictedBox.t(), tbox[layerIdx], x1y1x2y2 = False, GIoU = True) \n",
    "            # calculate GIoU box loss \n",
    "            GIoUBoxLoss += (1.0 - GIoU).mean()  \n",
    "\n",
    "            # calculate objectness value (GIoU ratio)\n",
    "            targetObj[imageIndex, anchor, gridY, gridX] = (1.0 - model.gr) + model.gr * GIoU.detach().clamp(0).type(targetObj.dtype)  \n",
    "\n",
    "            # calculate and sum BCE class loss\n",
    "            _targets = torch.full_like(predictionSubset[:, 5:], 0.0)  \n",
    "            _targets[range(numTargets), tcls[layerIdx]] = 1.0\n",
    "            classLoss += BCEcls(predictionSubset[:, 5:], _targets)  \n",
    "\n",
    "        # calculate and sum object loss \n",
    "        objectLoss += BCEobj(layerPrediction[..., 4], targetObj) \n",
    "\n",
    "    # finalise values for GIoU box loss using hyperparameters\n",
    "    GIoUBoxLoss *= model.hyp['giou']\n",
    "    # finalise values for object loss using hyperparameters\n",
    "    objectLoss *= model.hyp['obj']\n",
    "    # finalise values for class loss using hyperparameters\n",
    "    classLoss *= model.hyp['cls']\n",
    "\n",
    "    # calculate total loss \n",
    "    totLoss = GIoUBoxLoss + objectLoss + classLoss\n",
    "\n",
    "    return totLoss, torch.cat((GIoUBoxLoss, objectLoss, classLoss, totLoss)).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is called when computing loss. It works by iterating through the YOLO layers, and extracting the target anchors, classes, boxes, and relevant indices. The specifics are this function are shown in more detail in the block below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build targets for getLosses(), input targets(image,class,x,y,w,h)\n",
    "def buildTargets(prediction, targets, model):\n",
    "    \n",
    "    # extract number of targets \n",
    "    numTargets = targets.shape[0]\n",
    "    # init target classes, target boxes, target indices, target anchors to empty lists\n",
    "    targetClasses, targetBoxes, targetIndices, targetAnchors = [], [], [], []\n",
    "    # init gain to tensor filled with ones \n",
    "    gain = torch.ones(6, device = targets.device)\n",
    "\n",
    "    # iterate through each layer in YOLO's layers\n",
    "    for idx, layer in enumerate(model.yoloLayers):\n",
    "        # extract anchors in current layer \n",
    "        anchors = model.moduleList[layer].anchorVector\n",
    "        # extract number of anchors\n",
    "        numAnchors = anchors.shape[0]  \n",
    "        # create anchor tensor \n",
    "        anchorTensor = torch.arange(numAnchors).view(numAnchors, 1).repeat(1, numTargets)  \n",
    "        # calculate xyxy gain\n",
    "        gain[2:] = torch.tensor(prediction[idx].shape)[[3, 2, 3, 2]] \n",
    "\n",
    "        # match targets to anchors\n",
    "        \n",
    "        # init layer anchor indices list \n",
    "        layerAnchorIndices = []\n",
    "        # calculate scaled targets by multiplying by gain \n",
    "        scaledTargets = targets*gain\n",
    "        # init offsets\n",
    "        offsets = 0\n",
    "\n",
    "        # check if number of targets is larger than zero \n",
    "        if numTargets:\n",
    "            layer = widthHeightIOU(anchors, scaledTargets[:, 4:6]) > model.hyp['iou_t']  \n",
    "            layerAnchorIndices, scaledTargets = anchorTensor[layer], scaledTargets.repeat(numAnchors, 1, 1)[layer]  #\n",
    "            # overlaps\n",
    "            gridXY = scaledTargets[:, 2:4]  \n",
    "\n",
    "       # extract image index and image class\n",
    "        imageIndex, imageClass = scaledTargets[:, :2].long().T \n",
    "\n",
    "        # gridX, gridY, gridW, gridH respectively represent the x, y, w, h on the grid\n",
    "        # extract grid x,y values \n",
    "        gridXY = scaledTargets[:, 2:4]  \n",
    "        # extract grid w,h values \n",
    "        gridWH = scaledTargets[:, 4:6]  \n",
    "\n",
    "        # extract grid i,j values (grid x,y indices)\n",
    "        # gridI, gridJ represent the integer part of x, y (which grid on the current feature map) - coords of upper left corner on feature map\n",
    "        gridIJ = (gridXY - offsets).long()\n",
    "        gridI, gridJ = gridIJ.T  \n",
    "\n",
    "        # append values accordingly to corresponding lists \n",
    "        targetIndices.append((imageIndex, layerAnchorIndices, gridJ.clamp_(0, gain[3] - 1), gridI.clamp_(0, gain[2] - 1)))\n",
    "        targetBoxes.append(torch.cat((gridXY - gridIJ, gridWH), 1))  \n",
    "        targetAnchors.append(anchors[layerAnchorIndices]) \n",
    "        targetClasses.append(imageClass)  \n",
    "\n",
    "    return targetClasses, targetBoxes, targetIndices, targetAnchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Maximum Supression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issues surrounding object detection is that the algorithm may find multiple detections of the same object. Rather than detecting an object just once, it might detect it multiple times. Non-Max Suppression is a way for you to make sure that your algorithm detects each object only once.\n",
    "\n",
    "Non Maximum Suppression is a computer vision method that selects a single entity out of many overlapping entities (for example bounding boxes in object detection). The criteria is usually discarding entities that are below a given probability bound. With remaining entities we repeatedly pick the entity with the highest probability, output that as the prediction, and discard any remaining box where a IoU ≥ 0.5 with the box output in the previous step.\n",
    "\n",
    "This function works by [TALK ABOUT HOW THE CODE WORKS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def NMS(prediction, conf_thres = 0.1, iou_thres = 0.6, multi_label = True, classes = None, agnostic = False):\n",
    "    \"\"\"\n",
    "    Performs  Non-Maximum Suppression on inference results\n",
    "    Returns detections with shape:\n",
    "        nx6 (x1, y1, x2, y2, conf, cls)\n",
    "    \"\"\"\n",
    "\n",
    "    # init minimum and maximum width and height \n",
    "    minBoxWH, maxBoxWH = 2, 4096  \n",
    "    # extract number fo classes \n",
    "    numClasses = prediction[0].shape[1] - 5\n",
    "    # multiple labels per box \n",
    "    multi_label &= numClasses > 1\n",
    "    # init output list \n",
    "    output = [None] * prediction.shape[0]\n",
    "\n",
    "    # iterate through images in prediction\n",
    "    for imageIndex, imageInference in enumerate(prediction):  \n",
    "\n",
    "        # apply confidence thresholding constraints and filter out the images that have a confidence score below our min threshold value\n",
    "        imageInference = imageInference[imageInference[:, 4] > conf_thres]  \n",
    "        # apply widht-height thresholding constraints and filter out the images that do not fall within the range min-max\n",
    "        imageInference = imageInference[((imageInference[:, 2:4] > minBoxWH) & (imageInference[:, 2:4] < maxBoxWH)).all(1)]  # width-height\n",
    "\n",
    "        # check if there are no detections remaining after filtering\n",
    "        if not imageInference.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # calculate confidence score by multiplying object confidence and class confidence together \n",
    "        imageInference[..., 5:] *= imageInference[..., 4:5]  \n",
    "\n",
    "        # the bounding box attributes we have now are described by the center coordinates, as well as the height and width of the bounding box\n",
    "        # however it is easier to calculate IoU of two boxes, using coordinates of a pair of diagnal corners for each box. \n",
    "        # so we want to  transform the (center x, center y, height, width) attributes of our boxes, to (top-left corner x, top-left corner y,  right-bottom corner x, right-bottom corner y) aka (x1,y1,x2,y2)\n",
    "        box = xywh2xyxy(imageInference[:, :4])\n",
    "\n",
    "        # create an Nx6 detection matrix (xyxy, conf, cls)\n",
    "        nmsIndices, j = (imageInference[:, 5:] > conf_thres).nonzero().t()\n",
    "        imageInference = torch.cat((box[nmsIndices], imageInference[nmsIndices, j + 5].unsqueeze(1), j.float().unsqueeze(1)), 1)\n",
    "\n",
    "        # check if classes is not none \n",
    "        if classes:\n",
    "            # filter by classes\n",
    "            imageInference = imageInference[(j.view(-1, 1) == torch.tensor(classes, device = j.device)).any(1)]\n",
    "\n",
    "        # extract number of boxes\n",
    "        numBoxes = imageInference.shape[0]  \n",
    "\n",
    "        # check if there are no detections remaining after filtering\n",
    "        if not numBoxes:\n",
    "            continue\n",
    "\n",
    "        # Batched NMS\n",
    "        \n",
    "        # extract number of classes \n",
    "        c = imageInference[:, 5] * 0 if agnostic else imageInference[:, 5]  \n",
    "        # extract boxes offset by class and scores\n",
    "        boxes, scores = imageInference[:, :4].clone() + c.view(-1, 1) * maxBoxWH, imageInference[:, 4]  \n",
    "        # preform nms and store indices of elements to keep\n",
    "        nmsIndices = torchvision.ops.boxes.nms(boxes, scores, iou_thres)\n",
    "\n",
    "        # preform merge NMS using weighted mean \n",
    "        if (1 < numBoxes < 3E3):  \n",
    "            try:  \n",
    "                # create iou matrix \n",
    "                iou = boxIOU(boxes[nmsIndices], boxes) > iou_thres  # iou matrix\n",
    "                # calculate box weights \n",
    "                weights = iou * scores[None]  \n",
    "                # merge boxes \n",
    "                imageInference[nmsIndices, :4] = torch.mm(weights, imageInference[:, :4]).float() / weights.sum(1, keepdim = True)  \n",
    "            except: \n",
    "                print(imageInference, nmsIndices, imageInference.shape, nmsIndices.shape)\n",
    "                pass\n",
    "\n",
    "        output[imageIndex] = imageInference[nmsIndices]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is responsible for converting the output of the YOLO mode into a target list containing the batch index, class type, confidence score, and x/y/width/height values. It works by iterating through the output and extracting the relevant values and appending them to targets as a single list after each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToTarget(output, width, height):\n",
    "    \"\"\"\n",
    "    Convert a YOLO model output to target format\n",
    "    [batch_id, class_id, x, y, w, h, conf]\n",
    "    \"\"\"\n",
    "    # check if output is a PyTorch tensor and convert to numpy array \n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.cpu().numpy()\n",
    "    \n",
    "    # init targets list \n",
    "    targets = []\n",
    "    \n",
    "    # iterate through outputs \n",
    "    for index, currOutput in enumerate(output):\n",
    "        # check if current output is not empty \n",
    "        if currOutput is not None:\n",
    "            # iterate through predictions in current output \n",
    "            for prediction in currOutput:\n",
    "                # extract bounding box for current prediction\n",
    "                box = prediction[:4]\n",
    "                # extract width of bounding box \n",
    "                widthBox = (box[2] - box[0]) / width\n",
    "                # extract height of bounding box \n",
    "                heightBox = (box[3] - box[1]) / height\n",
    "                # extract x coordinate of bounding box\n",
    "                xBox = box[0] / width + widthBox / 2\n",
    "                # extract y coordinate of bounding box \n",
    "                yBox = box[1] / height + heightBox / 2\n",
    "                # extract confidence score \n",
    "                conf = prediction[4]\n",
    "                # extract box's predicted class \n",
    "                classID = int(prediction[5])\n",
    "                # append to targets \n",
    "                targets.append([index, classID, xBox, yBox, widthBox, heightBox, conf])\n",
    "\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function are standard plotting functions used to plot images, boxes, and training/testing results. The three functions utilise the cv2 and matplotlib libraries, though I will not delve into too much detail about their inner workings. The code is shown below with step by step comments for you to follow along with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotImages(images, targets, paths = None, fname ='images.jpg', names = None, max_size = 640, max_subplots = 16):\n",
    "    \n",
    "    # init line thickness \n",
    "    lineThickness = 3  \n",
    "    #  init font thickness\n",
    "    fontThickness = max(lineThickness - 1, 1)  \n",
    "    \n",
    "    # check if file arealdy exists and do not overrwrite \n",
    "    if os.path.isfile(fname):  \n",
    "        return None\n",
    "    # check if images are a PyTorch tensor and convert to numpy\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.cpu().numpy()\n",
    "    # check if targets are a PyTorch tensor and convert to numpy \n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.cpu().numpy()\n",
    "\n",
    "    # un-normalise images \n",
    "    if np.max(images[0]) <= 1:\n",
    "        images *= 255\n",
    "\n",
    "    # extract batchSize, height, width from image shape \n",
    "    batchSize, _, height, width = images.shape\n",
    "    # calculate batch size as min of batch size and the max number of subplots   \n",
    "    batchSize = min(batchSize, max_subplots)\n",
    "    # calculate number of square subplots \n",
    "    numSubPlots = np.ceil(batchSize ** 0.5)  \n",
    "\n",
    "    # calculate scale factor \n",
    "    scaleFactor = max_size / max(height, width)\n",
    "    # check if resizing is necessary \n",
    "    if scaleFactor < 1:\n",
    "        height = math.ceil(scaleFactor * height)\n",
    "        width = math.ceil(scaleFactor * width)\n",
    "\n",
    "    # init empty array for output\n",
    "    mosaic = np.full((int(numSubPlots * height), int(numSubPlots * width), 3), 255, dtype = np.uint8)\n",
    "\n",
    "    # craete class - colour lookup table \n",
    "    propertyCycle = plt.rcParams['axes.prop_cycle']\n",
    "    hex2rgb = lambda height: tuple(int(height[1 + index:1 + index + 2], 16) for index in (0, 2, 4))\n",
    "    colourLookUpTable = [hex2rgb(height) for height in propertyCycle.by_key()['color']]\n",
    "\n",
    "    # iterate through images\n",
    "    for index, img in enumerate(images):\n",
    "        \n",
    "        # check if we have reached max number of subplots \n",
    "        if index == max_subplots:  \n",
    "            break\n",
    "        \n",
    "        # calculate block x value\n",
    "        block_x = int(width * (index // numSubPlots))\n",
    "        # calculate block y value \n",
    "        block_y = int(height * (index % numSubPlots))\n",
    "        # transpose image accordingly \n",
    "        img = img.transpose(1, 2, 0)\n",
    "        \n",
    "        # check if image needs to be resized \n",
    "        if scaleFactor < 1:\n",
    "            img = cv2.resize(img, (width, height))\n",
    "\n",
    "        # assign image to mosaic \n",
    "        mosaic[block_y:block_y + height, block_x:block_x + width, :] = img\n",
    "        \n",
    "        # calculate number of targets \n",
    "        numTargets = len(targets) \n",
    "\n",
    "        # check if number of targets is larger than zero \n",
    "        if numTargets > 0:\n",
    "            # extract image targets\n",
    "            image_targets = targets[targets[:, 0] == index]\n",
    "            # extract bounding boxes \n",
    "            boxes = xywh2xyxy(image_targets[:, 2:6]).T\n",
    "            # extract classes \n",
    "            classes = image_targets[:, 1].astype('int')\n",
    "            # ground truth if no confidence column\n",
    "            groundTruth = image_targets.shape[1] == 6\n",
    "            # check for confidence precense \n",
    "            conf = None if groundTruth else image_targets[:, 6]  \n",
    "\n",
    "            boxes[[0, 2]] *= width\n",
    "            boxes[[0, 2]] += block_x\n",
    "            boxes[[1, 3]] *= height\n",
    "            boxes[[1, 3]] += block_y\n",
    "\n",
    "            # iterate through boxes \n",
    "            for j, box in enumerate(boxes.T):\n",
    "                # extract image class \n",
    "                imgCls = int(classes[j])\n",
    "                imgCls = names[imgCls] if names else imgCls\n",
    "                # extract colour from look-up table  \n",
    "                color = colourLookUpTable[imgCls % len(colourLookUpTable)]\n",
    "                \n",
    "                # confidence threshold \n",
    "                if groundTruth or conf[j] > 0.3:\n",
    "                    # extract label and plot box \n",
    "                    label = '%s' % imgCls if groundTruth else '%s %.1f' % (imgCls, conf[j])\n",
    "                    plotBox(box, mosaic, label = label, color = color, line_thickness = lineThickness)\n",
    "\n",
    "        # check if paths is not none and draw image filename labels\n",
    "        if paths is not None:\n",
    "            # trim label to fourty characters\n",
    "            label = os.path.basename(paths[index])[:40] \n",
    "            # get text size \n",
    "            textSize = cv2.getTextSize(label, 0, fontScale = lineThickness / 3, thickness = fontThickness)[0]\n",
    "            # add text to image \n",
    "            cv2.putText(mosaic, label, (block_x + 5, block_y + textSize[1] + 5), 0, lineThickness / 3, [220, 220, 220], thickness = fontThickness, lineType = cv2.LINE_AA)\n",
    "\n",
    "        # create image border\n",
    "        cv2.rectangle(mosaic, (block_x, block_y), (block_x + width, block_y + height), (255, 255, 255), thickness = 3)\n",
    "\n",
    "    # resize mosaic accordingly \n",
    "    mosaic = cv2.resize(mosaic, (int(numSubPlots * width * 0.5), int(numSubPlots * height * 0.5)), interpolation = cv2.INTER_AREA)\n",
    "    # save mosaic  \n",
    "    cv2.imwrite(fname, cv2.cvtColor(mosaic, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots one bounding box on image img\n",
    "def plotBox(x, img, color = None, label = None, line_thickness = None):\n",
    "    \n",
    "    # init line thickness\n",
    "    lineThickness = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1\n",
    "    # start point and sned point for rectangle \n",
    "    startPoint, endPoint = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    # draw rectangle on image \n",
    "    cv2.rectangle(img, startPoint, endPoint, color, thickness = lineThickness, lineType = cv2.LINE_AA)\n",
    "\n",
    "    # check if label is not none \n",
    "    if label:\n",
    "        # calculate font thickness \n",
    "        fontThickness = max(lineThickness - 1, 1)  \n",
    "        # calculate text size \n",
    "        textSize = cv2.getTextSize(label, 0, fontScale = lineThickness / 3, thickness = fontThickness)[0]\n",
    "        # cecalculate end point\n",
    "        endPoint = startPoint[0] + textSize[0], startPoint[1] - textSize[1] - 3\n",
    "        # draw rectangle for label and fill it \n",
    "        cv2.rectangle(img, startPoint, endPoint, color, -1, cv2.LINE_AA)  \n",
    "        # place text in rectangle \n",
    "        cv2.putText(img, label, (startPoint[0], startPoint[1] - 2), 0, lineThickness / 3, [225, 255, 255], thickness = fontThickness, lineType = cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(start = 0, stop = 0, bucket ='', id =()):  \n",
    "\n",
    "    # create list of graph titles \n",
    "    graphTitles = ['GIoU', 'Objectness', 'Classification', 'Precision', 'Recall', 'val GIoU', 'val Objectness', 'val Classification', 'mAP@0.5', 'F1']\n",
    "    # create figure, axis instance \n",
    "    figure, axis = plt.subplots(2, 5, figsize =(12, 6), tight_layout = True)\n",
    "    axis = axis.ravel()\n",
    "    # extract files\n",
    "    files = glob.glob('results*.txt') + glob.glob('../../Downloads/results*.txt')\n",
    "    \n",
    "    # iterate through files\n",
    "    for file in sorted(files):\n",
    "        # load text from file and assign to results\n",
    "        results = np.loadtxt(file, usecols =[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin = 2).T\n",
    "        # extract number of rows\n",
    "        numRows = results.shape[1] \n",
    "        x = numRows\n",
    "\n",
    "        for i in range(10):\n",
    "            y = results[i, x]\n",
    "\n",
    "            # do not show loss values of zero \n",
    "            if i in [0, 1, 2, 5, 6, 7]:\n",
    "                y[y == 0] = np.nan\n",
    "            \n",
    "            # plot and set title \n",
    "            axis[i].plot(x, y, marker ='.', label = Path(file).stem, linewidth = 2, markersize = 8)\n",
    "            axis[i].set_title(graphTitles[i])\n",
    "\n",
    "    # show legend \n",
    "    axis[1].legend()\n",
    "    # save figure as png\n",
    "    figure.savefig('results.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureConcat(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(FeatureConcat, self).__init__()\n",
    "        self.layerIndices = layers  \n",
    "        self.isMultipleLayers = len(layers) > 1  \n",
    "\n",
    "    def forward(self, x, outputs):\n",
    "        return torch.cat([outputs[i] for i in self.layerIndices], 1) if self.isMultipleLayers else outputs[self.layerIndices[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\n",
    "class WeightedFeatureFusion(nn.Module):  \n",
    "    def __init__(self, layers, weight = False):\n",
    "        super(WeightedFeatureFusion, self).__init__()\n",
    "        self.layerIndices = layers  \n",
    "        self.isApplyWeights = weight  \n",
    "        self.numLayers = len(layers) + 1 \n",
    "\n",
    "        if weight:\n",
    "            self.layerWeights = nn.Parameter(torch.zeros(self.numLayers), requires_grad = True)  \n",
    "\n",
    "    def forward(self, x, outputs):\n",
    "        if self.isApplyWeights:\n",
    "            w = torch.sigmoid(self.layerWeights) * (2 / self.numLayers)  \n",
    "            x = x * w[0]\n",
    "\n",
    "        inputChannels = x.shape[1]  \n",
    "        \n",
    "        for i in range(self.numLayers - 1):\n",
    "            addFeatures = outputs[self.layerIndices[i]] * w[i + 1] if self.isApplyWeights else outputs[self.layerIndices[i]]  \n",
    "            featureChannles = addFeatures.shape[1]  \n",
    "\n",
    "            if inputChannels == featureChannles:\n",
    "                x = x + addFeatures\n",
    "            elif inputChannels > featureChannles:  \n",
    "                x[:, :featureChannles] = x[:, :featureChannles] + addFeatures  \n",
    "            else:\n",
    "                x = x + addFeatures[:, :inputChannels]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLayer(nn.Module):\n",
    "    def __init__(self, anchors, numClasses, imageSize, yoloLayerIndex, layers, stride):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "\n",
    "        # init class variables\n",
    "        self.anchors = torch.Tensor(anchors)\n",
    "        self.layerIndex = yoloLayerIndex  \n",
    "        self.layerIndices = layers  \n",
    "        self.layerStride = stride  \n",
    "        self.numOutputLayers = len(layers)  \n",
    "        self.numAnchors = len(anchors) \n",
    "        self.numClasses = numClasses  \n",
    "        self.numOutputs = numClasses + 5  \n",
    "        self.numXGridPoints, self.numYGridPoints, self.numGridpoints = 0, 0, 0  \n",
    "        self.anchorVector = self.anchors / self.layerStride\n",
    "        self.anchorWH = self.anchorVector.view(1, self.numAnchors, 1, 1, 2)\n",
    "\n",
    "\n",
    "    def creatGrids(self, numGridPoints =(13, 13), device ='cpu'):\n",
    "        \n",
    "        # extract number of x, y gridpoints\n",
    "        self.numXGridPoints, self.numYGridPoints = numGridPoints  \n",
    "        # create gridpoints tensor\n",
    "        self.numGridpoints = torch.tensor(numGridPoints, dtype = torch.float)\n",
    "\n",
    "        # check if not currently training and build xy offsets \n",
    "        if not self.training:\n",
    "            yv, xv = torch.meshgrid([torch.arange(self.numYGridPoints, device = device), torch.arange(self.numXGridPoints, device = device)])\n",
    "            self.grid = torch.stack((xv, yv), 2).view((1, 1, self.numYGridPoints, self.numXGridPoints, 2)).float()\n",
    "\n",
    "        # check if devices do not match and send to device \n",
    "        if self.anchorVector.device != device:\n",
    "            self.anchorVector = self.anchorVector.to(device)\n",
    "            self.anchorWH = self.anchorWH.to(device)\n",
    "\n",
    "    def forward(self, prediction, out):\n",
    "        \n",
    "        # extract batch size, number of y gridpoints, number of x gridpoints\n",
    "        batchSize, _, numYGridPoints, numXGridPoints = prediction.shape  \n",
    "\n",
    "        # check if there is a mismatch in grid sizes and create grids\n",
    "        if (self.numXGridPoints, self.numYGridPoints) != (numXGridPoints, numYGridPoints):\n",
    "            self.creatGrids((numXGridPoints, numYGridPoints), prediction.device)\n",
    "\n",
    "        # reshape prediction accordingly \n",
    "        prediction = prediction.view(batchSize, self.numAnchors, self.numOutputs, self.numYGridPoints, self.numXGridPoints).permute(0, 1, 3, 4, 2).contiguous()  \n",
    "\n",
    "        # check if training is true\n",
    "        if self.training:\n",
    "            return prediction\n",
    "        \n",
    "        #inference \n",
    "        else:\n",
    "            # extract inference output\n",
    "            inferenceOutput = prediction.clone() \n",
    "            # xy calculation\n",
    "            inferenceOutput[..., :2] = torch.sigmoid(inferenceOutput[..., :2]) + self.grid  \n",
    "            # wh yolo method calculation\n",
    "            inferenceOutput[..., 2:4] = torch.exp(inferenceOutput[..., 2:4]) * self.anchorWH  \n",
    "            # multiply xywh by layer stride \n",
    "            inferenceOutput[..., :4] *= self.layerStride\n",
    "            # pass inferenceOutput[..., 4:] through sigmoid function\n",
    "            torch.sigmoid_(inferenceOutput[..., 4:])\n",
    "\n",
    "            # view [1, 3, 13, 13, 85] as [1, 507, 85]\n",
    "            return inferenceOutput.view(batchSize, -1, self.numOutputs), prediction  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv3 object detection model\n",
    "class Darknet(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, imageSize =(416, 416), verbose = False):\n",
    "        super(Darknet, self).__init__()\n",
    "\n",
    "        # init class variables\n",
    "        self.moduleDefinitions = parseModel(cfg)\n",
    "        self.moduleList, self.routs = createModules(self.moduleDefinitions, imageSize, cfg)\n",
    "        self.yoloLayers = [i for i, m in enumerate(self.moduleList) if m.__class__.__name__ == 'YOLOLayer']  \n",
    "        self.version = np.array([0, 2, 5], dtype = np.int32)  \n",
    "        self.numImageSeen = np.array([0], dtype = np.int64)  \n",
    "\n",
    "    def forward(self, x, augment = False, verbose = False):\n",
    "        \n",
    "        # check if augment is false \n",
    "        if not augment:\n",
    "            # pass x through Once\n",
    "            return self.forwardOnce(x)\n",
    "\n",
    "        else:  \n",
    "            # extract image size \n",
    "            imageSize = x.shape[-2:]  \n",
    "            # init scales\n",
    "            scales = [0.83, 0.67] \n",
    "            # init y list  \n",
    "            output = []\n",
    "\n",
    "            # iterate through x, flipped (left-right) and scaled x, scaled x \n",
    "            for i, xi in enumerate((x, torch_utils.scaleImage(x.flip(3), scales[0], same_shape = False),  torch_utils.scaleImage(x, scales[1], same_shape = False))):\n",
    "                # pass value through forward once and append output\n",
    "                output.append(self.forwardOnce(xi)[0])\n",
    "\n",
    "            # scale\n",
    "            output[1][..., :4] /= scales[0]  \n",
    "            # flip left-right\n",
    "            output[1][..., 0] = imageSize[1] - output[1][..., 0]  \n",
    "            # scale\n",
    "            output[2][..., :4] /= scales[1]\n",
    "\n",
    "            # concatenate output\n",
    "            output = torch.cat(output, 1)\n",
    "\n",
    "            return output, None\n",
    "\n",
    "    def forwardOnce(self, inferenceOutput):\n",
    "\n",
    "        # init list for yolo output and output \n",
    "        yoloLayerOutput, output = [], []\n",
    "\n",
    "        # iterate through modules in module list\n",
    "        for i, module in enumerate(self.moduleList):\n",
    "            # extract module class name \n",
    "            name = module.__class__.__name__\n",
    "\n",
    "            # check if module is of type WeightedFeatureFusion or FeatureConcat\n",
    "            if name in ['WeightedFeatureFusion', 'FeatureConcat']: \n",
    "                # extract inference output \n",
    "                inferenceOutput = module(inferenceOutput, output)  \n",
    "            \n",
    "            # check if module is of type YOLOLayer\n",
    "            elif name == 'YOLOLayer':\n",
    "                # extract and append yolo output \n",
    "                yoloLayerOutput.append(module(inferenceOutput, output))\n",
    "\n",
    "            else: \n",
    "                # extract inference output\n",
    "                inferenceOutput = module(inferenceOutput)\n",
    "\n",
    "            # append inference output to output list \n",
    "            output.append(inferenceOutput if self.routs[i] else [])\n",
    "\n",
    "        # check if training \n",
    "        if self.training:\n",
    "            return yoloLayerOutput\n",
    "        \n",
    "        # inference or test\n",
    "        else: \n",
    "            # extract inference and training output \n",
    "            inferenceOutput, trainingOutput = zip(*yoloLayerOutput)  \n",
    "            # concatenate yolo outputs \n",
    "            inferenceOutput = torch.cat(inferenceOutput, 1)  \n",
    "\n",
    "            return inferenceOutput, trainingOutput\n",
    "\n",
    "    # Fuse Conv2d + BatchNorm2d layers throughout model\n",
    "    def fuse(self):\n",
    "        # init instance of nn.ModuleList()\n",
    "        fuseList = nn.ModuleList()\n",
    "        \n",
    "        # iterate through child modules\n",
    "        for child in list(self.children())[0]:\n",
    "           \n",
    "            # check if child is of type nn.Sequential\n",
    "            if isinstance(child, nn.Sequential):\n",
    "           \n",
    "                # iterate throguh child\n",
    "                for index, val in enumerate(child):\n",
    "           \n",
    "                    # check if current val is of type nn.modules.batchnorm.BatchNorm2d\n",
    "                    if isinstance(val, nn.modules.batchnorm.BatchNorm2d):\n",
    "                        # fuse this bn layer with the previous conv2d layer\n",
    "                        conv = child[index - 1]\n",
    "                        fused = torch_utils.fuseConvBnLayers(conv, val)\n",
    "                        child = nn.Sequential(fused, *list(child.children())[index + 1:])\n",
    "                        break\n",
    "            \n",
    "            # append child to fused list \n",
    "            fuseList.append(child)\n",
    "        # assign module list to fuse list \n",
    "        self.moduleList = fuseList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseModel(path):\n",
    "    # init empty lists\n",
    "    moduleDefinitions, validLines = [], []\n",
    "    # read cfg file line by line and store it\n",
    "    allLines = open(path, 'r').read().split('\\n')\n",
    "    \n",
    "    for line in allLines:\n",
    "        # check if line is not empty and do not start with '#'\n",
    "        if line and not line.startswith(\"#\"):\n",
    "            # append line and strip all fringe whitespace \n",
    "            validLines.append(line.rstrip().lstrip())\n",
    "\n",
    "    for line in validLines:\n",
    "        # check if we are at the start of a new block \n",
    "        isNewBlock = line.startswith('[')\n",
    "        \n",
    "        if isNewBlock:\n",
    "            # append and populate a dictionary to moduleDefinitions\n",
    "            moduleDefinitions.append({})\n",
    "            moduleDefinitions[-1]['type'] = line[1:-1].rstrip()\n",
    "            # check if module type is convolutional and add batch norm parameter\n",
    "            if moduleDefinitions[-1]['type'] == 'convolutional':\n",
    "                # pre-populate with zeros (may be overwritten later)\n",
    "                moduleDefinitions[-1]['batch_normalize'] = 0  \n",
    "        \n",
    "        else:\n",
    "            # extract key, value pair\n",
    "            key, val = line.split(\"=\")\n",
    "            # strip whitespace \n",
    "            key = key.rstrip()\n",
    "\n",
    "            # return a numpy array \n",
    "            if key == 'anchors':  \n",
    "                moduleDefinitions[-1][key] = np.array([float(x) for x in val.split(',')]).reshape((-1, 2))\n",
    "            # return a regular array \n",
    "            elif (key in ['from', 'layers', 'mask']):  \n",
    "                moduleDefinitions[-1][key] = [int(x) for x in val.split(',')]\n",
    "            # return a regular array \n",
    "            elif (key == 'size' and ',' in val): \n",
    "                moduleDefinitions[-1][key] = [int(x) for x in val.split(',')]\n",
    "\n",
    "            else:\n",
    "                # strip whitespace \n",
    "                val = val.strip()\n",
    "                # return int/float \n",
    "                if val.isnumeric():\n",
    "                    moduleDefinitions[-1][key] = int(val) if (int(val) - float(val)) == 0 else float(val)   # return int or float\n",
    "                # return string \n",
    "                else:\n",
    "                    moduleDefinitions[-1][key] = val  \n",
    "\n",
    "    return moduleDefinitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(path):\n",
    "    # init output dictionary \n",
    "    options = dict()\n",
    "\n",
    "    # open are read data file into lines \n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # strip whitespace \n",
    "        line = line.strip()\n",
    "        # check if line is empty or starts with a '#' (indicates a comment)\n",
    "        if line == '' or line.startswith('#'): continue\n",
    "        # extract key, value pair \n",
    "        key, val = line.split('=')\n",
    "        # add key,value pair to dictionary \n",
    "        options[key.strip()] = val.strip()\n",
    "\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructs module list of layer blocks from module configuration in moduleDefinitions\n",
    "def createModules(moduleDefinitions, imgSize, cfg):\n",
    "    \n",
    "    # check if image size is an integer or tuple, and expand it if necessary \n",
    "    imgSize = [imgSize] * 2 if isinstance(imgSize, int) else imgSize  \n",
    "    # extract hyperparameters from config file (unused)\n",
    "    trainingHyperparms = moduleDefinitions.pop(0)  \n",
    "    # init output filters\n",
    "    outputFilters = [3]  \n",
    "    # init module list \n",
    "    moduleList = nn.ModuleList()\n",
    "    # init routing layers (list of layers that route to deeper layers)\n",
    "    routingLayers = []  \n",
    "    # init yolo index\n",
    "    yoloIndex = -1\n",
    "\n",
    "    # iterate through parsed cfg file and construct modules \n",
    "    for idx, currModule in enumerate(moduleDefinitions):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if currModule['type'] == 'convolutional':\n",
    "            # extract batch normalize value \n",
    "            isBatchNormalize = currModule['batch_normalize']\n",
    "            # extract filters value \n",
    "            filters = currModule['filters']\n",
    "            # extract kernel size value\n",
    "            kernelSize = currModule['size']   \n",
    "            # extract stride value\n",
    "            stride = currModule['stride'] if 'stride' in currModule else (currModule['stride_y'], currModule['stride_x'])\n",
    "            # create convolutional layer \n",
    "            convLayer = nn.Conv2d(in_channels = outputFilters[-1], out_channels = filters, kernel_size = kernelSize, stride = stride, padding = kernelSize // 2 if currModule['pad'] else 0, groups = currModule['groups'] if 'groups' in currModule else 1, bias = not isBatchNormalize)\n",
    "            # add convolutional layer \n",
    "            modules.add_module('Conv2d', convLayer)\n",
    "\n",
    "            # if batch normalise \n",
    "            if isBatchNormalize:\n",
    "                # create batch norm layer \n",
    "                batchNormLayer = nn.BatchNorm2d(filters, momentum = 0.03, eps = 1E-4)\n",
    "                # add batch norm layer \n",
    "                modules.add_module('BatchNorm2d', batchNormLayer)\n",
    "            \n",
    "            else:\n",
    "                # detection output that goes into YOLO layer \n",
    "                routingLayers.append(idx)  \n",
    "\n",
    "            # leaky activation\n",
    "            if currModule['activation'] == 'leaky':  \n",
    "                # creat leaky layer \n",
    "                leakyLayer = nn.LeakyReLU(0.1, inplace = True)\n",
    "                # add leaky layer \n",
    "                modules.add_module('activation', leakyLayer)\n",
    "\n",
    "        elif currModule['type'] == 'upsample':\n",
    "            # create upsample layer \n",
    "            modules = nn.Upsample(scale_factor = currModule['stride'])\n",
    "\n",
    "        elif currModule['type'] == 'route':  \n",
    "            # extract layers \n",
    "            layers = currModule['layers']\n",
    "            # extract filters \n",
    "            filters = sum([outputFilters[l + 1 if l > 0 else l] for l in layers])\n",
    "            # extend routing layers\n",
    "            routingLayers.extend([idx + l if l < 0 else l for l in layers])\n",
    "            # creat route layer using FeatureConcat class\n",
    "            modules = FeatureConcat(layers = layers)\n",
    "\n",
    "        elif currModule['type'] == 'shortcut':\n",
    "            # extract layers \n",
    "            layers = currModule['from']\n",
    "            # extract filters \n",
    "            filters = outputFilters[-1]\n",
    "            # extend routing layers \n",
    "            routingLayers.extend([idx + l if l < 0 else l for l in layers])\n",
    "            # create shortcut layer using WeightedFeatureFusion class\n",
    "            modules = WeightedFeatureFusion(layers = layers, weight ='weights_type' in currModule)\n",
    "\n",
    "        elif currModule['type'] == 'yolo':\n",
    "            # increment yolo index \n",
    "            yoloIndex += 1\n",
    "            # init stride list \n",
    "            stride = [32, 16, 8]\n",
    "            # extract layers \n",
    "            layers = currModule['from'] if 'from' in currModule else []\n",
    "            # create yolo layer \n",
    "            modules = YOLOLayer(anchors = currModule['anchors'][currModule['mask']], numClasses = currModule['classes'],   imageSize = imgSize,   yoloLayerIndex = yoloIndex,   layers = layers, stride = stride[yoloIndex])\n",
    "\n",
    "            # init preceding Conv2d() bias \n",
    "            j = layers[yoloIndex] if 'from' in currModule else -1\n",
    "            bias_ = moduleList[j][0].bias  \n",
    "            bias = bias_[:modules.numOutputs * modules.numAnchors].view(modules.numAnchors, -1)  \n",
    "            bias[:, 4] += -4.5  \n",
    "            bias[:, 5:] += math.log(0.6 / (modules.numClasses - 0.99))  \n",
    "            moduleList[j][0].bias = torch.nn.Parameter(bias_, requires_grad = bias_.requires_grad)\n",
    "\n",
    "        # append modules to module list \n",
    "        moduleList.append(modules)\n",
    "        # append filters to output filters \n",
    "        outputFilters.append(filters)\n",
    "    \n",
    "    # init binary routing layers \n",
    "    binaryRoutingLayers = [False] * (idx + 1)\n",
    "    # iterate through routing layers and set those indices to true in binary routing layers list \n",
    "    for idx in routingLayers:\n",
    "        binaryRoutingLayers[idx] = True\n",
    "\n",
    "    return moduleList, binaryRoutingLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses and loads the weights stored in 'weights'\n",
    "def loadDarkNetWeights(self, weights, cutoff=-1):\n",
    "\n",
    "    # Establish cutoffs (load layers between 0 and cutoff. if cutoff = -1 all are loaded)\n",
    "    file = Path(weights).name\n",
    "    if file == 'darknet53.conv.74':\n",
    "        cutoff = 75\n",
    "    elif file == 'yolov3-tiny.conv.15':\n",
    "        cutoff = 15\n",
    "\n",
    "    # Read weights file\n",
    "    with open(weights, 'rb') as f:\n",
    "        self.version = np.fromfile(f, dtype=np.int32, count=3)  # (int32) version info: major, minor, revision\n",
    "        self.seen = np.fromfile(f, dtype=np.int64, count=1)  # (int64) number of images seen during training\n",
    "\n",
    "        weights = np.fromfile(f, dtype=np.float32)  # the rest are weights\n",
    "\n",
    "    ptr = 0\n",
    "    for idx, (moduleDef, module) in enumerate(zip(self.moduleDefinitions[:cutoff], self.moduleList[:cutoff])):\n",
    "        if moduleDef['type'] == 'convolutional':\n",
    "            # extract conv\n",
    "            conv = module[0]\n",
    "\n",
    "            # load batch normalization bias, weights, running mean and running variance\n",
    "            if moduleDef['batch_normalize']:\n",
    "\n",
    "                # extract batch normalize \n",
    "                batchNormalize = module[1]\n",
    "\n",
    "                # extract number of biases\n",
    "                numBiases = batchNormalize.bias.numel() \n",
    "\n",
    "                # load bias\n",
    "                batchNormBias = torch.from_numpy(weights[ptr:ptr + numBiases])\n",
    "                \n",
    "                batchNormalize.bias.data.copy_(torch.from_numpy(weights[ptr:ptr + numBiases]).view_as(batchNormalize.bias))\n",
    "                ptr += numBiases\n",
    "\n",
    "                # load weight\n",
    "                batchNormWeight = torch.from_numpy(weights[ptr:ptr + numBiases])\n",
    "                \n",
    "                batchNormalize.weight.data.copy_(torch.from_numpy(weights[ptr:ptr + numBiases]).view_as(batchNormalize.weight))\n",
    "                ptr += numBiases\n",
    "\n",
    "                # load running mean \n",
    "                batchNormRunMean = torch.from_numpy(weights[ptr:ptr + numBiases])\n",
    "\n",
    "                batchNormalize.running_mean.data.copy_(torch.from_numpy(weights[ptr:ptr + numBiases]).view_as(batchNormalize.running_mean))\n",
    "                ptr += numBiases\n",
    "\n",
    "                # load running var \n",
    "                batchNormRunVar = torch.from_numpy(weights[ptr:ptr + numBiases])\n",
    "\n",
    "                batchNormalize.running_var.data.copy_(torch.from_numpy(weights[ptr:ptr + numBiases]).view_as(batchNormalize.running_var))\n",
    "                ptr += numBiases\n",
    "\n",
    "                # cast into dimensions of the model \n",
    "                batchNormBias = batchNormBias.view_as(batchNormalize.bias)\n",
    "                batchNormWeight = batchNormWeight.view_as(batchNormalize.weight)\n",
    "                batchNormRunMean = batchNormRunMean.view_as(batchNormalize.running_mean)\n",
    "                batchNormRunVar = batchNormRunVar.view_as(batchNormalize.running_var)\n",
    "\n",
    "                # copy data to model \n",
    "                batchNormalize.bias.data.copy_(batchNormBias)\n",
    "                batchNormalize.weight.data.copy_(batchNormWeight)\n",
    "                batchNormalize.running_mean.data.copy_(batchNormRunMean)\n",
    "                batchNormalize.running_var.data.copy_(batchNormRunVar)\n",
    "\n",
    "            else:\n",
    "                # extract number of biases \n",
    "                numBiases = conv.bias.numel()\n",
    "                # load bias \n",
    "                convBias = torch.from_numpy(weights[ptr:ptr + numBiases])\n",
    "                # cast into dimensions of model\n",
    "                convBias = convBias.view_as(conv.bias)\n",
    "                # copy data to model \n",
    "                conv.bias.data.copy_(convBias)\n",
    "                # increment pointer \n",
    "                ptr += numBiases\n",
    "\n",
    "            # load conv weights\n",
    "            # extract number of weights \n",
    "            numWeights = conv.weight.numel()  \n",
    "            # load weights \n",
    "            convWeights = torch.from_numpy(weights[ptr:ptr + numWeights])\n",
    "            # cast into dimensions of model \n",
    "            convWeights = convWeights.view_as(conv.weight)\n",
    "            # copy data to model \n",
    "            conv.weight.data.copy_(convWeights)\n",
    "            # increment pointer \n",
    "            ptr += numWeights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuseConvBnLayers(conv, bn):\n",
    "\n",
    "    # disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # crate fused convolutional layer \n",
    "        fusedconv = torch.nn.Conv2d(conv.in_channels, conv.out_channels, kernel_size = conv.kernel_size, stride = conv.stride, padding = conv.padding, bias = True)\n",
    "        # extract convolutional weights\n",
    "        convolutionalWeights = conv.weight.clone().view(conv.out_channels, -1)\n",
    "        # extract batch normalization weights \n",
    "        batchNormWeights = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n",
    "        # init and reshape fused convolutional layer weights \n",
    "        fusedconv.weight.copy_(torch.mm(batchNormWeights, convolutionalWeights).view(fusedconv.weight.size()))\n",
    "\n",
    "        # check if convolutional layer bias is not none \n",
    "        if conv.bias is not None:\n",
    "            # extract convolutional spatial bias \n",
    "            convolutionalBias = conv.bias\n",
    "        else:\n",
    "            # set to zero tensor \n",
    "            convolutionalBias = torch.zeros(conv.weight.size(0))\n",
    "        \n",
    "        # extract batch normalization spatial bias\n",
    "        batchNormBias = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "        # init and reshape fused convolutional layer bias\n",
    "        fusedconv.bias.copy_(torch.mm(batchNormWeights, convolutionalBias.reshape(-1, 1)).reshape(-1) + batchNormBias)\n",
    "\n",
    "        return fusedconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
