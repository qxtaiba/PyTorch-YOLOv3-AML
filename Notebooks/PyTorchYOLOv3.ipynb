{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1610750408829",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loadImage() is used to load a single image from the COCO dataset and returns the original image, its height and width, as well as its resized height and width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(self, index):\n",
    "    img = self.imgs[index]\n",
    "    path = self.img_files[index]\n",
    "    img = cv2.imread(path)  \n",
    "    originalHeight, originalWidth = img.shape[:2]\n",
    "    r = self.img_size / max(originalHeight, originalWidth)  # resize image to img_size\n",
    "    if r != 1:  # always resize down, only resize up if training with augmentation\n",
    "        interp = cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR\n",
    "        img = cv2.resize(img, (int(originalWidth * r), int(originalHeight * r)), interpolation = interp)\n",
    "    return img, (originalHeight, originalWidth), img.shape[:2]  # img, hw_original, hw_resized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "augmentHSV() is used to modify an input image inplace by manipulating its hue, saturation, and value. \n",
    "\n",
    "Hue, saturation, and value are the main color properties that allow us to distinguish between different colors. \n",
    "\n",
    "1. Hues are the three primary colors (red, blue, and yellow) and the three secondary colors (orange, green, and violet) that appear in the color wheel or color circle. When you refer to hue, you are referring to pure color, or the visible spectrum of basic colors that can be seen in a rainbow. \n",
    "\n",
    "2. Color saturation is the purity and intensity of a color as displayed in an image. The higher the saturation of a color, the more vivid and intense it is. The lower a color’s saturation, or chroma, the closer it is to pure gray on the grayscale.\n",
    "\n",
    "3. Color value refers to the relative lightness or darkness of a color. We perceive color value based on the quantity of light reflected off of a surface and absorbed by the human eye. We refer to the intensity of the light that reaches the eye as “luminance.”\n",
    "\n",
    "\n",
    "Modifying these values allows us to augment our input image, expand out dataset, and improve our training results.\n",
    "\n",
    "The LUT OpenCV function applies a lookup-table transformation using calculate values for hue, saturation, and value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentHSV(img, hgain = 0.5, sgain = 0.5, vgain = 0.5):\n",
    "    randomGains = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n",
    "    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
    "\n",
    "    x = np.arange(0, 256, dtype = np.int16)\n",
    "    lut_hue = ((x * randomGains[0]) % 180).astype(img.dtype)\n",
    "    lut_sat = np.clip(x * randomGains[1], 0, 255).astype(img.dtype)\n",
    "    lut_val = np.clip(x * randomGains[2], 0, 255).astype(img.dtype)\n",
    "\n",
    "    img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(img.dtype)\n",
    "    cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst = img)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loadMosaic() is used to load images into a mosaic of four. It is a form of augmentation that is used only during training. It works by taking a total of four images, creating a base image with the corresponding number of tiles, and then calculating the position of each image on the base image. It also calculates the requried padding, normalizes the image labels, and then concatenates/clips the labels and applies an augmentation to both the images and labels. It returns the modified images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mosaic(self, index):\n",
    "\n",
    "    labels4 = []\n",
    "    s = self.imageSize\n",
    "    centerX, centerY = [int(random.uniform(s * 0.5, s * 1.5)) for _ in range(2)]\n",
    "    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]  # 3 additional image indices\n",
    "    \n",
    "    for i, index in enumerate(indices):\n",
    "        # Load image\n",
    "        img, _, (h, w) = loadImage(self, index)\n",
    "\n",
    "        # top left\n",
    "        if i == 0: \n",
    "            # base image with 4 tiles\n",
    "            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype = np.uint8)  \n",
    "\n",
    "            # xmin, ymin, xmax, ymax (large image)\n",
    "            x1a, y1a, x2a, y2a = max(centerX - w, 0), max(centerY - h, 0), centerX, centerY\n",
    "\n",
    "            # xmin, ymin, xmax, ymax (small image)\n",
    "            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  \n",
    "        \n",
    "        # top right\n",
    "        elif i == 1:  \n",
    "\n",
    "            # xmin, ymin, xmax, ymax (large image)\n",
    "            x1a, y1a, x2a, y2a = centerX, max(centerY - h, 0), min(centerX + w, s * 2), centerY\n",
    "\n",
    "            # xmin, ymin, xmax, ymax (small image)\n",
    "            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "        \n",
    "        # bottom left\n",
    "        elif i == 2:\n",
    "\n",
    "            # xmin, ymin, xmax, ymax (large image)\n",
    "            x1a, y1a, x2a, y2a = max(centerX - w, 0), centerY, centerX, min(s * 2, centerY + h)\n",
    "\n",
    "            # xmin, ymin, xmax, ymax (small image)\n",
    "            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(centerX, w), min(y2a - y1a, h)\n",
    "        \n",
    "        # bottom right\n",
    "        elif i == 3:\n",
    "\n",
    "            # xmin, ymin, xmax, ymax (large image)\n",
    "            x1a, y1a, x2a, y2a = centerX, centerY, min(centerX + w, s * 2), min(s * 2, centerY + h)\n",
    "\n",
    "            # xmin, ymin, xmax, ymax (small image)\n",
    "            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "\n",
    "        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  \n",
    "        \n",
    "        widthPadding = x1a - x1b\n",
    "        heightPadding = y1a - y1b\n",
    "\n",
    "        # labels\n",
    "        x = self.labels[index]\n",
    "        labels = x.copy()\n",
    "\n",
    "        # normalize xywh to pixel xyxy format\n",
    "        if x.size > 0:\n",
    "            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2) + widthPadding\n",
    "            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2) + heightPadding\n",
    "            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2) + widthPadding\n",
    "            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2) + heightPadding\n",
    "        \n",
    "        labels4.append(labels)\n",
    "\n",
    "    # concatenate and clip labels\n",
    "    if len(labels4):\n",
    "        labels4 = np.concatenate(labels4, 0)\n",
    "        np.clip(labels4[:, 1:], 0, 2 * s, out = labels4[:, 1:])  # use with random_affine\n",
    "\n",
    "    # augment images+labels\n",
    "    img4, labels4 = random_affine(img4, labels4,degrees = self.hyp['degrees'], translate = self.hyp['translate'], scale = self.hyp['scale'], shear = self.hyp['shear'], border =-s // 2)  # border to remove\n",
    "\n",
    "    return img4, labels4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "letterbox() is used to resize an input image into a 32-pixel-multiple rectangle. This reduces inference time proportionally to the amount of letterboxed area padded onto a square image. It works by extracting the current shape, calculating the neccessary padding, resizing it if necessary, and then creating and adding a border. It returns the letterboxed image, the scaling ratio, as well as the padding used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, newShape = (416, 416), color =(114, 114, 114), auto = True, scaleFill = False, scaleup = True):\n",
    "    \n",
    "    # extract current shape\n",
    "    currShape = img.shape[:2]\n",
    "\n",
    "    if isinstance(newShape, int):\n",
    "        newShape = (newShape, newShape)\n",
    "\n",
    "    # scale ratio (new / old)\n",
    "    scaleRatio = min(newShape[0] / currShape[0], newShape[1] / currShape[1])\n",
    "\n",
    "    # only scale down, do not scale up\n",
    "    if not scaleup:\n",
    "        scaleRatio = min(scaleRatio, 1.0)\n",
    "\n",
    "    # calculate padding\n",
    "    ratio = scaleRatio, scaleRatio\n",
    "    unpaddedShape = int(round(currShape[1] * scaleRatio)), int(round(currShape[0] * scaleRatio))\n",
    "    widthPadding, heightPadding = newShape[1] - unpaddedShape[0], newShape[0] - unpaddedShape[1]  \n",
    "    \n",
    "    if auto:  \n",
    "        widthPadding, heightPadding = np.mod(widthPadding, 32), np.mod(heightPadding, 32)  # wh padding\n",
    "\n",
    "    widthPadding /= 2\n",
    "    heightPadding /= 2\n",
    "\n",
    "    if currShape[::-1] != unpaddedShape:\n",
    "        img = cv2.resize(img, unpaddedShape, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    # create and add border\n",
    "    top, bottom = int(round(heightPadding - 0.1)), int(round(heightPadding + 0.1))\n",
    "    left, right = int(round(widthPadding - 0.1)), int(round(widthPadding + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value = color)  \n",
    "\n",
    "    return img, ratio, (widthPadding, heightPadding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomAffine() is another form of dataset augmentation used to apply rotate, scale, translate, and shear transforms to an input image. It also transforms the label coordinates and returns the image and label coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_affine(img, targets =(), degrees = 10, translate =.1, scale =.1, shear = 10, border = 0):\n",
    "\n",
    "    height = img.shape[0] + border * 2\n",
    "    width = img.shape[1] + border * 2\n",
    "\n",
    "    # rotate and scale\n",
    "    R = np.eye(3)\n",
    "    a = random.uniform(-degrees, degrees)\n",
    "    s = random.uniform(1 - scale, 1 + scale)\n",
    "    R[:2] = cv2.getRotationMatrix2D(angle = a, center =(img.shape[1] / 2, img.shape[0] / 2), scale = s)\n",
    "\n",
    "    # translate\n",
    "    T = np.eye(3)\n",
    "    T[0, 2] = random.uniform(-translate, translate) * img.shape[0] + border  # x translation (pixels)\n",
    "    T[1, 2] = random.uniform(-translate, translate) * img.shape[1] + border  # y translation (pixels)\n",
    "\n",
    "    # shear\n",
    "    S = np.eye(3)\n",
    "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n",
    "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n",
    "\n",
    "    # combined rotation matrix\n",
    "    M = S @ T @ R\n",
    "    if (border != 0) or (M != np.eye(3)).any():  # image changed\n",
    "        img = cv2.warpAffine(img, M[:2], dsize =(width, height), flags = cv2.INTER_LINEAR, borderValue =(114, 114, 114))\n",
    "\n",
    "    # transform label coordinates\n",
    "    if len(targets):\n",
    "\n",
    "        # warp points\n",
    "        xy = np.ones((len(targets) * 4, 3))\n",
    "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(len(targets) * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "        xy = (xy @ M.T)[:, :2].reshape(len(targets), 8)\n",
    "\n",
    "        # create new boxes\n",
    "        x = xy[:, [0, 2, 4, 6]]\n",
    "        y = xy[:, [1, 3, 5, 7]]\n",
    "        xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, len(targets)).T\n",
    "\n",
    "        # reject warped points outside of image\n",
    "        xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n",
    "        xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n",
    "        \n",
    "        w = xy[:, 2] - xy[:, 0]\n",
    "        h = xy[:, 3] - xy[:, 1]\n",
    "        \n",
    "        area = w * h\n",
    "        area0 = (targets[:, 3] - targets[:, 1]) * (targets[:, 4] - targets[:, 2])\n",
    "        \n",
    "        aspectRatio = np.maximum(w / (h + 1e-16), h / (w + 1e-16))  # aspect ratio\n",
    "        \n",
    "        i = (w > 4) & (h > 4) & (area / (area0 * s + 1e-16) > 0.2) & (aspectRatio < 10)\n",
    "\n",
    "        targets = targets[i]\n",
    "        targets[:, 1:5] = xy[i]\n",
    "\n",
    "    return img, targets\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class defines a set of functions used to generate a list of images and their corresponding labels when training and/or testing the network. The image(s) and their corresponding label(s) can be found in data/coco/trainvalno5k.txt and data/coco/5k.txt. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-466-6be8053771b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLoadImagesAndLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class LoadImagesAndLabels(Dataset):  \n",
    "    def __init__(self, path, img_size=416, batch_size=16, augment=False, hyp=None, image_weights=False, cache_images=False, single_cls=False, pad=0.0):\n",
    "        path = str(Path(path))  \n",
    "        parent = str(Path(path).parent) + os.sep\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            f = f.read().splitlines()\n",
    "            f = [x.replace('./', parent) if x.startswith('./') else x for x in f] \n",
    "\n",
    "        self.imgFiles = [x.replace('/', os.sep) for x in f if os.path.splitext(x)[-1].lower() in img_formats]\n",
    "\n",
    "        numFiles = len(self.imgFiles)\n",
    "        batchIndex = np.floor(np.arange(numFiles) / batch_size).astype(np.int)  \n",
    "\n",
    "        self.numImages = numFiles  \n",
    "        self.batch = batchIndex  \n",
    "        self.imageSize = img_size\n",
    "        self.isAugment = augment\n",
    "        self.hyperparameters = hyp\n",
    "        self.isImageWeights = image_weights\n",
    "        self.isMosaic = self.isAugment   \n",
    "\n",
    "        # define labels\n",
    "        self.labelFiles = [x.replace('images', 'labels').replace(os.path.splitext(x)[-1], '.txt')\n",
    "                            for x in self.imgFiles]\n",
    "\n",
    "        widthHeight = [getEXIFsize(Image.open(f)) for f in self.imgFiles]\n",
    "\n",
    "        self.shapes = np.array(widthHeight, dtype=np.float64)\n",
    "\n",
    "        # cache labels\n",
    "        self.imgs = [None] * numFiles\n",
    "        self.labels = [np.zeros((0, 5), dtype=np.float32)] * numFiles\n",
    "        \n",
    "        widthHeight = path.replace('images', 'labels')\n",
    "\n",
    "        for i, file in enumerate(self.labelFiles):\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    l = np.array([x.split() for x in f.read().splitlines()], dtype=np.float32)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if l.shape[0]:\n",
    "                self.labels[i] = l\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgFiles)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hyp = self.hyperparameters\n",
    "\n",
    "        # create mosaic\n",
    "        if self.isMosaic:\n",
    "            img, labels = load_mosaic(self, index)\n",
    "            shapes = None\n",
    "\n",
    "        else:\n",
    "\n",
    "            # load image \n",
    "            img, (h0, w0), (h, w) = loadImage(self, index)\n",
    "\n",
    "            # letterbox\n",
    "            shape = self.imgSize\n",
    "            img, ratio, pad = letterbox(img, shape, auto = False, scaleup = self.isAugment)\n",
    "            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n",
    "\n",
    "            # load labels\n",
    "            labels = []\n",
    "            x = self.labels[index]\n",
    "\n",
    "            # normalize xywh to xyxy format\n",
    "            if x.size > 0:\n",
    "                labels = x.copy()\n",
    "                labels[:, 1] = ratio[0] * w * (x[:, 1] - x[:, 3] / 2) + pad[0]  \n",
    "                labels[:, 2] = ratio[1] * h * (x[:, 2] - x[:, 4] / 2) + pad[1]  \n",
    "                labels[:, 3] = ratio[0] * w * (x[:, 1] + x[:, 3] / 2) + pad[0]\n",
    "                labels[:, 4] = ratio[1] * h * (x[:, 2] + x[:, 4] / 2) + pad[1]\n",
    "\n",
    "        # augment image/color space\n",
    "        if self.isAugment:\n",
    "            if not self.isMosaic:\n",
    "                img, labels = random_affine(img, labels, degrees = hyp['degrees'], translate = hyp['translate'], scale = hyp['scale'], shear = hyp['shear'])\n",
    "            augmentHSV(img, hgain = hyp['hsv_h'], sgain = hyp['hsv_s'], vgain = hyp['hsv_v'])\n",
    "        \n",
    "        # convert xyxy to xywh and normalize coordinates\n",
    "        if len(labels) :\n",
    "            labels[:, 1:5] = xyxy2xywh(labels[:, 1:5])\n",
    "\n",
    "            # normalize height \n",
    "            labels[:, [2, 4]] /= img.shape[0]  \n",
    "\n",
    "            # normalize width \n",
    "            labels[:, [1, 3]] /= img.shape[1]  \n",
    "\n",
    "        # random left-right flip\n",
    "        if self.isAugment:\n",
    "            if random.random() < 0.5:\n",
    "                img = np.fliplr(img)\n",
    "                if len(labels) :\n",
    "                    labels[:, 1] = 1 - labels[:, 1]\n",
    "\n",
    "        outputLabels = torch.zeros((len(labels) , 6))\n",
    "        if len(labels) :\n",
    "            outputLabels[:, 1:] = torch.from_numpy(labels)\n",
    "\n",
    "        # convert from BGR to RGB and reshape to 3x416x416\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  \n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return torch.from_numpy(img), outputLabels, self.imgFiles[index], shapes\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "\n",
    "        img, label, path, shapes = zip(*batch)  \n",
    "\n",
    "        # add target image index for buildTargets()\n",
    "        for i, l in enumerate(label):\n",
    "            l[:, 0] = i\n",
    "\n",
    "        return torch.stack(img, 0), torch.cat(label, 0), path, shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImages: \n",
    "    def __init__(self, path, img_size = 416):\n",
    "        path = str(Path(path))  \n",
    "        files = []\n",
    "\n",
    "        if os.path.isdir(path):\n",
    "            files = sorted(glob.glob(os.path.join(path, '*.*')))\n",
    "        elif os.path.isfile(path):\n",
    "            files = [path]\n",
    "\n",
    "        images = [x for x in files if os.path.splitext(x)[-1].lower() in img_formats]\n",
    "\n",
    "        numImages = len(images)\n",
    "\n",
    "        self.imgSize = img_size\n",
    "        self.files = images \n",
    "        self.numFiles = numImages \n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count == self.numFiles:\n",
    "            raise StopIteration\n",
    "        \n",
    "        # extract path\n",
    "        path = self.files[self.count]\n",
    "\n",
    "        # increment count\n",
    "        self.count += 1\n",
    "\n",
    "        # read image\n",
    "        img0 = cv2.imread(path)\n",
    "\n",
    "        # resize by adding padding\n",
    "        img = letterbox(img0, newShape = self.imgSize)[0]\n",
    "\n",
    "        # convert image from BGR to RGB and to 3x416x416\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return path, img, img0\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # return number of files\n",
    "        return self.numFiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to parse the cfg, and store every block as a dict. The attributes of the blocks and their values are stored as key-value pairs in the dictionary. As we parse through the cfg, we keep appending these dicts, denoted by the variable block in our code, to a list blocks. Our function will return this block.\n",
    "\n",
    "We begin by saving the content of the cfg file in a list of strings. Then, we loop over the resultant list to get blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseModel(path):\n",
    "\n",
    "    # init empty lists\n",
    "    moduleDefinitions, validLines = [], []\n",
    "\n",
    "    # read cfg file line by line and store it\n",
    "    allLines = open(path, 'r').read().split('\\n')\n",
    "    \n",
    "    # extract and append all lines that are not empty and do not start with '#'\n",
    "    for line in allLines:\n",
    "        if line and not line.startswith(\"#\"):\n",
    "            validLines.append(line.rstrip().lstrip())\n",
    "\n",
    "    for line in validLines:\n",
    "        \n",
    "        # check if we are at the start of a new block \n",
    "        isNewBlock = line.startswith('[')\n",
    "\n",
    "        if isNewBlock:\n",
    "\n",
    "            # append and populate a dictionary to moduleDefinitions\n",
    "            moduleDefinitions.append({})\n",
    "            moduleDefinitions[-1]['type'] = line[1:-1].rstrip()\n",
    "\n",
    "            # check if module type is convolutional and add batch norm parameter\n",
    "            if moduleDefinitions[-1]['type'] == 'convolutional':\n",
    "                moduleDefinitions[-1]['batch_normalize'] = 0  # pre-populate with zeros (may be overwritten later)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # extract key, value pair\n",
    "            key, val = line.split(\"=\")\n",
    "\n",
    "            # strip whitespace \n",
    "            key = key.rstrip()\n",
    "\n",
    "            if key == 'anchors':  \n",
    "                moduleDefinitions[-1][key] = np.array([float(x) for x in val.split(',')]).reshape((-1, 2))\n",
    "\n",
    "            elif (key in ['from', 'layers', 'mask']):  \n",
    "                moduleDefinitions[-1][key] = [int(x) for x in val.split(',')]\n",
    "            \n",
    "            elif (key == 'size' and ',' in val): \n",
    "                moduleDefinitions[-1][key] = [int(x) for x in val.split(',')]\n",
    "\n",
    "            else:\n",
    "                val = val.strip()\n",
    "                if val.isnumeric():\n",
    "                    moduleDefinitions[-1][key] = int(val) if (int(val) - float(val)) == 0 else float(val)   # return int or float\n",
    "                else:\n",
    "                    moduleDefinitions[-1][key] = val  \n",
    "\n",
    "    return moduleDefinitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(path):\n",
    "    options = dict()\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '' or line.startswith('#'): continue\n",
    "        key, val = line.split('=')\n",
    "        options[key.strip()] = val.strip()\n",
    "\n",
    "    return options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function allows us to parse and load weights into our model. The first 160 bytes of the weights file store 5 int32 values which constitute the header of the file. The rest of bits now represent the weights and are stored as float32 or 32-bit floats. They are loaded in a np.ndarray and we then we iterate over the weights file and load the weights into the modules of our network.\n",
    "\n",
    "Into the loop, we first check whether the convolutional block has batch_normalise True or not. Based on that, we load the weights. We keep a variable called ptr to keep track of where we are in the weights array. Now, if batch_normalize is True, we load the weights. If batch_norm is not true, simply load the biases of the convolutional layer. Finally, we load the convolutional layer's weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDarkNetWeights(self, weights, cutoff=-1):\n",
    "    # Parses and loads the weights stored in 'weights'\n",
    "\n",
    "    # Establish cutoffs (load layers between 0 and cutoff. if cutoff = -1 all are loaded)\n",
    "    file = Path(weights).name\n",
    "    cutoff = 75\n",
    "\n",
    "    # Read weights file\n",
    "    with open(weights, 'rb') as f:\n",
    "        self.version = np.fromfile(f, dtype=np.int32, count=3)  # (int32) version info: major, minor, revision\n",
    "        self.seen = np.fromfile(f, dtype=np.int64, count=1)  # (int64) number of images seen during training\n",
    "        weights = np.fromfile(f, dtype=np.float32)  # the rest are weights\n",
    "\n",
    "    ptr = 0\n",
    "    for idx, (moduleDef, module) in enumerate(zip(self.moduleDefinitions[:cutoff], self.module_list[:cutoff])):\n",
    "        if moduleDef['type'] == 'convolutional':\n",
    "            conv = module[0]\n",
    "            if moduleDef['batch_normalize']:\n",
    "                # Load BN bias, weights, running mean and running variance\n",
    "                bn = module[1]\n",
    "                nb = bn.bias.numel()  # number of biases\n",
    "                # Bias\n",
    "                bn.bias.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.bias))\n",
    "                ptr += nb\n",
    "                # Weight\n",
    "                bn.weight.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.weight))\n",
    "                ptr += nb\n",
    "                # Running Mean\n",
    "                bn.running_mean.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.running_mean))\n",
    "                ptr += nb\n",
    "                # Running Var\n",
    "                bn.running_var.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.running_var))\n",
    "                ptr += nb\n",
    "            else:\n",
    "                # Load conv. bias\n",
    "                nb = conv.bias.numel()\n",
    "                conv_b = torch.from_numpy(weights[ptr:ptr + nb]).view_as(conv.bias)\n",
    "                conv.bias.data.copy_(conv_b)\n",
    "                ptr += nb\n",
    "            # Load conv. weights\n",
    "            nw = conv.weight.numel()  # number of weights\n",
    "            conv.weight.data.copy_(torch.from_numpy(weights[ptr:ptr + nw]).view_as(conv.weight))\n",
    "            ptr += nw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLayers(model):\n",
    "    return [i for i, m in enumerate(model.moduleList) if m.__class__.__name__ == 'YOLOLayer']  # [89, 101, 113]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the building blocks of this network, we utilise the output of the parsing function to construct a number of PyTorch modules as dictated by the cfg list. This function essentially iterates over the list of blocks and creates a PyTorch module for each block as we go. The output of this function will return a list that contains an nn.Module object called an nn.ModuleList. The nn.Sequential class is also used to sequentially execute a number of nn.Module objects. This is useful as some blocks may contain more than one layer. nn.Sequential allows us to attatch these layers together.  \n",
    "\n",
    "In constructing the network, there are six main module types to consider. They are as follows:\n",
    "\n",
    "1. **Convolutional Layer**\n",
    "\n",
    "    The convolutional layer is a layer that contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter.\n",
    "\n",
    "    In defining the convolutional layer(s), it is important to define the kernel dimensions. This is pretty much taken care of by the parameters included in the cfg file, however we must also keep track of the number of filters present in each previous layer, therefore providing us with the depth of the feature map. It is also important to add the batch normalize layer, pad layer, as well as the leaky activation function. \n",
    "\n",
    "        \n",
    "        \n",
    "2. **Maxpool Layer**\n",
    "\n",
    "    The Max Pooling layer is a pooling operation that extracts the maximum value in each patch of a feature map, and then down-samples it to highlight the most present feature in the patch. It is essentially a sample-based discretization process that aims to reduce dimensionality and allow for assumptions to be made about features contained in the sub-regions. In YOLOv3, however, max pooling is not used for downsampling. Instead, a 3X3 convolutional kernel is used with a step size of 2. This takes place a total of five times. \n",
    "\n",
    "\n",
    "3. **Upsample Layer**\n",
    "\n",
    "    The upsample layer upsamples the feature map in the previous layer by a factor of stride using bilinear upsampling.\n",
    "    \n",
    "   \n",
    "\n",
    "4. **Route Layer**\n",
    "\n",
    "    The route layer plays a very important role in this network. It preforms the equivalent role of fusing together previous feature maps. If its attribute layer has only one value, then the route layer will output the feature maps of the layer indexed by that value. If, however, the attribute layer has two values then it will output the concatenated feature maps of the layers indexed by it's values along the depth dimension. It is also important to note that if there is a convolutional layer present right in front of a route layer, then the kernel is applied on the feature maps of previous layers, precisely the ones the route layer brings. Therefore, we need to keep a track of the number of filters in not only the previous layer, but each one of the preceding layers. As we iterate, we append the number of output filters of each block to the output_filters variable.\n",
    "\n",
    "\n",
    "\n",
    "5. **Shortcut Layer**\n",
    "\n",
    "    The shortcut layer is used to optimise the large network structure in order to provide faster training times and better convergence scores. It is essentially a skip connection that superimposes the value of the network without changing the size of the feature map, and so you will find that the input and output sizes have not changed before and after the shortcut layer. The output of the shortcut layer is found by adding feature maps.\n",
    "\n",
    "\n",
    "6. **YOLO Layer**\n",
    "    \n",
    "   The YOLO layer corresponds to the detection layer. YOLOv3 makes prediction across 3 different scales. The detection layer is used make detection at feature maps of three different sizes, having strides 32, 16, 8 respectively. This means, with an input of 416 x 416, we make detections on scales 13 x 13, 26 x 26 and 52 x 52. Generally, stride of any layer in the network is equal to the factor by which the output of the layer is smaller than the input image to the network.\n",
    "\n",
    "    The network downsamples the input image until the first detection layer, where a detection is made using feature maps of a layer with stride 32. Further, layers are upsampled by a factor of 2 and concatenated with feature maps of a previous layers having identical feature map sizes. Another detection is now made at layer with stride 16. The same upsampling procedure is repeated, and a final detection is made at the layer of stride 8. At each scale, each cell predicts 3 bounding boxes using 3 anchors, making the total number of anchors used 9. (The anchors are different for different scales). This is at the heart of YOLOv3's multi-scale detection idea. The use of 3 scales is to strengthen the detection of small targets. A relatively large feature map is used to detect relatively small targets, and a small feature map is responsible for detecting large targets.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModules(moduleDefinitions, imgSize, cfg):\n",
    "    # Constructs module list of layer blocks from module configuration in moduleDefinitions\n",
    "\n",
    "    imgSize = [imgSize] * 2 if isinstance(imgSize, int) else imgSize  # expand if necessary\n",
    "    trainingHyperparms = moduleDefinitions.pop(0)  # cfg training hyperparams (unused)\n",
    "    outputFilters = [3]  # input channels\n",
    "    moduleList = nn.ModuleList()\n",
    "    routingLayers = []  # list of layers which rout to deeper layers\n",
    "    yoloIndex = -1\n",
    "\n",
    "    for idx, currModule in enumerate(moduleDefinitions):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if currModule['type'] == 'convolutional':\n",
    "            isBatchNormalize = currModule['batch_normalize']\n",
    "            filters = currModule['filters']\n",
    "            kernelSize = currModule['size']  # kernel size\n",
    "            stride = currModule['stride'] if 'stride' in currModule else (currModule['stride_y'], currModule['stride_x'])\n",
    "            \n",
    "            modules.add_module('Conv2d', nn.Conv2d(in_channels = outputFilters[-1], out_channels = filters, kernel_size = kernelSize, stride = stride, padding = kernelSize // 2 if currModule['pad'] else 0, groups = currModule['groups'] if 'groups' in currModule else 1, bias = not isBatchNormalize))\n",
    "\n",
    "            if isBatchNormalize:\n",
    "                modules.add_module('BatchNorm2d', nn.BatchNorm2d(filters, momentum = 0.03, eps = 1E-4))\n",
    "            else:\n",
    "                routingLayers.append(idx)  # detection output (goes into yolo layer)\n",
    "\n",
    "            if currModule['activation'] == 'leaky':  \n",
    "                modules.add_module('activation', nn.LeakyReLU(0.1, inplace = True))\n",
    "\n",
    "        elif currModule['type'] == 'upsample':\n",
    "            modules = nn.Upsample(scale_factor = currModule['stride'])\n",
    "\n",
    "        elif currModule['type'] == 'route':  # nn.Sequential() placeholder for 'route' layer\n",
    "            layers = currModule['layers']\n",
    "            filters = sum([outputFilters[l + 1 if l > 0 else l] for l in layers])\n",
    "            routingLayers.extend([idx + l if l < 0 else l for l in layers])\n",
    "            modules = FeatureConcat(layers = layers)\n",
    "\n",
    "        elif currModule['type'] == 'shortcut':  # nn.Sequential() placeholder for 'shortcut' layer\n",
    "            layers = currModule['from']\n",
    "            filters = outputFilters[-1]\n",
    "            routingLayers.extend([idx + l if l < 0 else l for l in layers])\n",
    "            modules = WeightedFeatureFusion(layers = layers, weight ='weights_type' in currModule)\n",
    "\n",
    "        elif currModule['type'] == 'yolo':\n",
    "            yoloIndex += 1\n",
    "            stride = [32, 16, 8]  # P5, P4, P3 strides\n",
    "            layers = currModule['from'] if 'from' in currModule else []\n",
    "            modules = YOLOLayer(anchors = currModule['anchors'][currModule['mask']],  # anchor list\n",
    "                                numClasses = currModule['classes'],  # number of classes\n",
    "                                img_size = imgSize,  # (416, 416)\n",
    "                                yoloLayerIndex = yoloIndex,  # 0, 1, 2...\n",
    "                                layers = layers,  # output layers\n",
    "                                stride = stride[yoloIndex])\n",
    "\n",
    "            # Initialize preceding Conv2d() bias (https://arxiv.org/pdf/1708.02002.pdf section 3.3)\n",
    "            j = layers[yoloIndex] if 'from' in currModule else -1\n",
    "\n",
    "            bias_ = moduleList[j][0].bias  # shape(255,)\n",
    "            bias = bias_[:modules.numOutputs * modules.numAnchors].view(modules.numAnchors, -1)  # shape(3,85)\n",
    "            bias[:, 4] += -4.5  # obj\n",
    "            bias[:, 5:] += math.log(0.6 / (modules.numClasses - 0.99))  # cls (sigmoid(p) = 1/numClasses)\n",
    "            moduleList[j][0].bias = torch.nn.Parameter(bias_, requires_grad = bias_.requires_grad)\n",
    "\n",
    "\n",
    "        # Register module list and number of output filters\n",
    "        moduleList.append(modules)\n",
    "        outputFilters.append(filters)\n",
    "\n",
    "    binaryRoutingLayers = [False] * (idx + 1)\n",
    "    for idx in routingLayers:\n",
    "        binaryRoutingLayers[idx] = True\n",
    "    return moduleList, binaryRoutingLayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FeatureConcat class allows us to concatenate multiple feature maps. It is used as an alternative to XX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-472-7a0cc9724100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFeatureConcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureConcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misMultipleLayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class FeatureConcat(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(FeatureConcat, self).__init__()\n",
    "        self.layerIndices = layers  \n",
    "        self.isMultipleLayers = len(layers) > 1  \n",
    "\n",
    "    def forward(self, x, outputs):\n",
    "        return torch.cat([outputs[i] for i in self.layerIndices], 1) if self.isMultipleLayers else outputs[self.layerIndices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WeightedFeatureFusion class allows us to produce a weighted sum of two or more layers. It is used as an alternative to XX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-473-791d5c220c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mWeightedFeatureFusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWeightedFeatureFusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misApplyWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class WeightedFeatureFusion(nn.Module):  # weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\n",
    "    def __init__(self, layers, weight = False):\n",
    "        super(WeightedFeatureFusion, self).__init__()\n",
    "        self.layerIndices = layers  \n",
    "        self.isApplyWeights = weight  \n",
    "        self.numLayers = len(layers) + 1 \n",
    "\n",
    "        if weight:\n",
    "            self.layerWeights = nn.Parameter(torch.zeros(self.numLayers), requires_grad = True)  \n",
    "\n",
    "    def forward(self, x, outputs):\n",
    "        if self.isApplyWeights:\n",
    "            w = torch.sigmoid(self.layerWeights) * (2 / self.numLayers)  \n",
    "            x = x * w[0]\n",
    "\n",
    "        inputChannels = x.shape[1]  \n",
    "        \n",
    "        for i in range(self.numLayers - 1):\n",
    "            addFeatures = outputs[self.layerIndices[i]] * w[i + 1] if self.isApplyWeights else outputs[self.layerIndices[i]]  \n",
    "            featureChannles = addFeatures.shape[1]  \n",
    "\n",
    "            if inputChannels == featureChannles:\n",
    "                x = x + addFeatures\n",
    "            elif inputChannels > featureChannles:  \n",
    "                x[:, :featureChannles] = x[:, :featureChannles] + addFeatures  \n",
    "            else:\n",
    "                x = x + addFeatures[:, :inputChannels]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is for the YOLO Detection Layer. The forward pass is pretty important because it essentialy does the job of predict_transform. It makes everyting one tensor and does the equations that are provided here and earlier. This can be seen in this block of code where we pass through the sigmoid. There's also something about calculating grid offseats that is prertty important but I forgot lol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-474-02253d2e2a69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mYOLOLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYOLOLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class YOLOLayer(nn.Module):\n",
    "    def __init__(self, anchors, numClasses, img_size, yoloLayerIndex, layers, stride):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.anchors = torch.Tensor(anchors)\n",
    "        self.layerIndex = yoloLayerIndex  \n",
    "        self.layerIndices = layers  \n",
    "        self.layerStride = stride  \n",
    "        self.numOutputLayers = len(layers)  \n",
    "        self.numAnchors = len(anchors) \n",
    "        self.numClasses = numClasses  \n",
    "        self.numOutputs = numClasses + 5  \n",
    "        self.numX, self.numY, self.numGridpoints = 0, 0, 0  \n",
    "        self.anchorVector = self.anchors / self.layerStride\n",
    "        self.anchorWH = self.anchorVector.view(1, self.numAnchors, 1, 1, 2)\n",
    "\n",
    "\n",
    "    def creatGrids(self, ng =(13, 13), device ='cpu'):\n",
    "        self.numX, self.numY = ng  \n",
    "        self.numGridpoints = torch.tensor(ng, dtype = torch.float)\n",
    "\n",
    "        if not self.training:\n",
    "            yv, xv = torch.meshgrid([torch.arange(self.numY, device = device), torch.arange(self.numX, device = device)])\n",
    "            self.grid = torch.stack((xv, yv), 2).view((1, 1, self.numY, self.numX, 2)).float()\n",
    "\n",
    "        if self.anchorVector.device != device:\n",
    "            self.anchorVector = self.anchorVector.to(device)\n",
    "            self.anchorWH = self.anchorWH.to(device)\n",
    "\n",
    "    def forward(self, prediction, out):\n",
    "\n",
    "        bs, _, ny, nx = prediction.shape  # bs, 255, 13, 13\n",
    "        if (self.numX, self.numY) != (nx, ny):\n",
    "            self.creatGrids((nx, ny), prediction.device)\n",
    "\n",
    "        prediction = prediction.view(bs, self.numAnchors, self.numOutputs, self.numY, self.numX).permute(0, 1, 3, 4, 2).contiguous()  \n",
    "\n",
    "        if self.training:\n",
    "            return prediction\n",
    "\n",
    "        else:\n",
    "            inferenceOutput = prediction.clone() \n",
    "            inferenceOutput[..., :2] = torch.sigmoid(inferenceOutput[..., :2]) + self.grid  # xy\n",
    "            inferenceOutput[..., 2:4] = torch.exp(inferenceOutput[..., 2:4]) * self.anchorWH  # wh yolo method\n",
    "            inferenceOutput[..., :4] *= self.layerStride\n",
    "            torch.sigmoid_(inferenceOutput[..., 4:])\n",
    "            return inferenceOutput.view(bs, -1, self.numOutputs), prediction  # view [1, 3, 13, 13, 85] as [1, 507, 85]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Darknet class, we leverage a number of methods in order to construct the network's architecture block by block. In the init() function, we parse the configuration file and generate the relevant network architecture according to the content and order of the file. \n",
    "\n",
    "Talk about forward, forwardOnce.\n",
    "\n",
    "The fuse function's purpose is to fuse together all the Conv2d and BatchNorm2d layers throughout model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-475-96018c6a89b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# YOLOv3 object detection model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDarknet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Darknet(nn.Module):\n",
    "    # YOLOv3 object detection model\n",
    "\n",
    "    def __init__(self, cfg, img_size =(416, 416), verbose = False):\n",
    "        super(Darknet, self).__init__()\n",
    "\n",
    "        self.moduleDefinitions = parseModel(cfg)\n",
    "        self.moduleList, self.routs = createModules(self.moduleDefinitions, img_size, cfg)\n",
    "        self.yoloLayers = getLayers(self)\n",
    "        self.version = np.array([0, 2, 5], dtype = np.int32)  \n",
    "        self.numImageSeen = np.array([0], dtype = np.int64)  \n",
    "\n",
    "    def forward(self, x, augment = False, verbose = False):\n",
    "\n",
    "        if not augment:\n",
    "            return self.forwardOnce(x)\n",
    "        else:  \n",
    "            imageSize = x.shape[-2:]  \n",
    "            scales = [0.83, 0.67]  # scales\n",
    "            y = []\n",
    "            for i, xi in enumerate((x, torch_utils.scaleImage(x.flip(3), scales[0], same_shape = False),  torch_utils.scaleImage(x, scales[1], same_shape = False))):\n",
    "                y.append(self.forwardOnce(xi)[0])\n",
    "\n",
    "            y[1][..., :4] /= scales[0]  # scale\n",
    "            y[1][..., 0] = imageSize[1] - y[1][..., 0]  # flip lr\n",
    "            y[2][..., :4] /= scales[1]  # scale\n",
    "\n",
    "            y = torch.cat(y, 1)\n",
    "\n",
    "            return y, None\n",
    "\n",
    "    def forwardOnce(self, inferenceOutput, augment = False):\n",
    "        imageSize = inferenceOutput.shape[-2:]  \n",
    "        yoloLayerOutput, output = [], []\n",
    "\n",
    "        for i, module in enumerate(self.moduleList):\n",
    "            name = module.__class__.__name__\n",
    "            if name in ['WeightedFeatureFusion', 'FeatureConcat']: \n",
    "                inferenceOutput = module(inferenceOutput, output)  \n",
    "            elif name == 'YOLOLayer':\n",
    "                yoloLayerOutput.append(module(inferenceOutput, output))\n",
    "            else: \n",
    "                inferenceOutput = module(inferenceOutput)\n",
    "\n",
    "            output.append(inferenceOutput if self.routs[i] else [])\n",
    "\n",
    "        if self.training:\n",
    "            return yoloLayerOutput\n",
    "        else: \n",
    "            inferenceOutput, trainingOutput = zip(*yoloLayerOutput)  \n",
    "            inferenceOutput = torch.cat(inferenceOutput, 1)  \n",
    "            if augment:  \n",
    "                # de-augment results\n",
    "                inferenceOutput = torch.split(inferenceOutput, nb, dim=0)\n",
    "                # scale\n",
    "                inferenceOutput[1][..., :4] /= s[0]\n",
    "                # flip lr\n",
    "                inferenceOutput[1][..., 0] = imageSize[1] - inferenceOutput[1][..., 0] \n",
    "                # scale\n",
    "                inferenceOutput[2][..., :4] /= s[1]  \n",
    "\n",
    "                inferenceOutput = torch.cat(inferenceOutput, 1)\n",
    "\n",
    "            return inferenceOutput, trainingOutput\n",
    "\n",
    "    def fuse(self):\n",
    "        # Fuse Conv2d + BatchNorm2d layers throughout model\n",
    "        fuseList = nn.ModuleList()\n",
    "        for a in list(self.children())[0]:\n",
    "            if isinstance(a, nn.Sequential):\n",
    "                for i, b in enumerate(a):\n",
    "                    if isinstance(b, nn.modules.batchnorm.BatchNorm2d):\n",
    "                        # fuse this bn layer with the previous conv2d layer\n",
    "                        conv = a[i - 1]\n",
    "                        fused = torch_utils.fuseConvBnLayers(conv, b)\n",
    "                        a = nn.Sequential(fused, *list(a.children())[i + 1:])\n",
    "                        break\n",
    "            fuseList.append(a)\n",
    "        self.moduleList = fuseList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuseConvBnLayers(conv, bn):\n",
    "    with torch.no_grad():\n",
    "        fusedconv = torch.nn.Conv2d(conv.in_channels, conv.out_channels, kernel_size = conv.kernel_size, stride = conv.stride, padding = conv.padding, bias = True)\n",
    "\n",
    "        convolutionalWeights = conv.weight.clone().view(conv.out_channels, -1)\n",
    "        batchNormWeights = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n",
    "        fusedconv.weight.copy_(torch.mm(batchNormWeights, convolutionalWeights).view(fusedconv.weight.size()))\n",
    "\n",
    "        if conv.bias is not None:\n",
    "            convolutionalBias = conv.bias\n",
    "        else:\n",
    "            convolutionalBias = torch.zeros(conv.weight.size(0))\n",
    "        \n",
    "        batchNormBias = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "        fusedconv.bias.copy_(torch.mm(batchNormWeights, convolutionalBias.reshape(-1, 1)).reshape(-1) + batchNormBias)\n",
    "\n",
    "        return fusedconv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaleImg() is used to scale and resize an input image. It works by extracting the image's width and height, calculating the new size, and adding the necessary padding required to the tensor using torch.nn.functional.pad().  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleImage(img, ratio = 1.0, same_shape = True):  # img(16,3,256,416), r = ratio\n",
    "    \n",
    "    # extract width and height \n",
    "    height, width = img.shape[2:]\n",
    "\n",
    "    # calculate new size \n",
    "    newSize = (int(height * ratio), int(width * ratio))  \n",
    "\n",
    "    # resize image\n",
    "    img = F.interpolate(img, size = newSize, mode ='bilinear', align_corners = False)  \n",
    "    \n",
    "    return F.pad(img, [0, width - newSize[1], 0, height - newSize[0]], value = 0.447)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xyxy2xywh() converts boxes from  (x1, y1, x2, y2) to (x, y, w, h) where x1, y1 represent the top-left coordinates and x2, y2 represent the bottom-right coordinates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
    "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
    "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
    "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xywh2xyxy() converts boxes from (x, y, w, h) to (x1, y1, x2, y2) where x1, y1 represent the top-left coordinates and x2, y2 represent the bottom-right coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaleCoords is used to rescale the coordinates extracted from the shape of img1 to that of the shape of img0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleCoordinates(img1_shape, coords, img0_shape, ratio_pad = None):\n",
    "    gain = max(img1_shape) / max(img0_shape)  # gain  = old / new\n",
    "    pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "\n",
    "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
    "    coords[:, 0].clamp_(0, img0_shape[1])  # x1\n",
    "    coords[:, 1].clamp_(0, img0_shape[0])  # y1\n",
    "    coords[:, 2].clamp_(0, img0_shape[1])  # x2\n",
    "    coords[:, 3].clamp_(0, img0_shape[0])  # y2\n",
    "\n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apPerClass() compute the average precision, given the recall and precision curves. It takes in a numpy array of true positives, objectness value, predicted object classes, and true object classes in order to return the average precision on a class basis. More information about this function can be found here: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "\n",
    "It works by first sorting by objectness and extracting the sorted indices and findinf the unique class. It then creates the precision-recall curves and computes the AP for each class.\n",
    "\n",
    "In order to calculate the precision-recall curve, the function iterates through the unique classes and calculates the number of ground truth objects and the number of predicted objects. If these values are larger than 0, we accumulate the False/True positives and use them to calculate the recall and precision. \n",
    "\n",
    "The equations for this calculation are shown below: \n",
    "\n",
    "In order to calculate the average precision, please see the next block. \n",
    "\n",
    "In order to calculate the F1 score, the following equation is used: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAPClass(tp, conf, pred_cls, target_cls):\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes = np.unique(target_cls)\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    pr_score = 0.1  # score to evaluate P and R https://github.com/ultralytics/yolov3/issues/898\n",
    "    s = [unique_classes.shape[0], tp.shape[1]]  # number class, number iou thresholds (i.e. 10 for mAP0.5...0.95)\n",
    "    ap, p, r = np.zeros(s), np.zeros(s), np.zeros(s)\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
    "        n_p = i.sum()  # Number of predicted objects\n",
    "\n",
    "        if n_p == 0 or n_gt == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = (1 - tp[i]).cumsum(0)\n",
    "            tpc = tp[i].cumsum(0)\n",
    "\n",
    "            # Recall\n",
    "            recall = tpc / (n_gt + 1e-16)  # recall curve\n",
    "            r[ci] = np.interp(-pr_score, -conf[i], recall[:, 0])  # r at pr_score, negative x, xp because xp decreases\n",
    "\n",
    "            # Precision\n",
    "            precision = tpc / (tpc + fpc)  # precision curve\n",
    "            p[ci] = np.interp(-pr_score, -conf[i], precision[:, 0])  # p at pr_score\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            for j in range(tp.shape[1]):\n",
    "                ap[ci, j] = getAP(recall[:, j], precision[:, j])\n",
    "\n",
    "    # Compute F1 score (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + 1e-16)\n",
    "\n",
    "    return p, r, ap, f1, unique_classes.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computeAP() works alongside the function above. It takes in the recall and precision curves as lists in order to calculate and return the average precision. More information about this function can be found here: https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "It works by appending sentinel values to the beginning and end of the recall/precision lists, and then computing the precision envelope and integrating the area under the curve. The result of this integration is teh average precision and is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAP(recall, precision):\n",
    "\n",
    "    # Append sentinel values to beginning and end\n",
    "    mrec = np.concatenate(([0.], recall, [min(recall[-1] + 1E-3, 1.)]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # Compute the precision envelope\n",
    "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
    "\n",
    "    # Integrate area under curve\n",
    "    x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "    ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n",
    "\n",
    "    return ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingBoxIOU(firstBox, secondBox, x1y1x2y2 = True, GIoU = False):\n",
    "    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
    "    secondBox = secondBox.t()\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
    "        # Transform from center and width to exact coordinates\n",
    "        firstBoxX1, firstBoxY1, firstBoxX2, firstBoxY2 = firstBox[0], firstBox[1], firstBox[2], firstBox[3]\n",
    "        secondBoxX1, secondBoxY1, secondBoxX2, secondBoxY2 = secondBox[0], secondBox[1], secondBox[2], secondBox[3]\n",
    "    else:  # transform from xywh to xyxy\n",
    "        # Get the coordinates of bounding boxes\n",
    "        firstBoxX1, firstBoxX2 = firstBox[0] - firstBox[2] / 2, firstBox[0] + firstBox[2] / 2\n",
    "        firstBoxY1, firstBoxY2 = firstBox[1] - firstBox[3] / 2, firstBox[1] + firstBox[3] / 2\n",
    "        secondBoxX1, secondBoxX2 = secondBox[0] - secondBox[2] / 2, secondBox[0] + secondBox[2] / 2\n",
    "        secondBoxY1, secondBoxY2 = secondBox[1] - secondBox[3] / 2, secondBox[1] + secondBox[3] / 2\n",
    "\n",
    "    # extract intersection rectangle coordinates\n",
    "    rectIntersectionX1, rectIntersectionY1  = torch.max(firstBoxX1, secondBoxX1), torch.max(firstBoxY1, secondBoxY1) \n",
    "    rectIntersectionX2, rectIntersectionY2 = torch.min(firstBoxX2, secondBoxX2), torch.min(firstBoxY2, secondBoxY2)\n",
    "    \n",
    "    # Intersection area\n",
    "    intersectionWidth = (rectIntersectionX2 - rectIntersectionX1).clamp(0)\n",
    "    intersectionHeight = (rectIntersectionY2 - rectIntersectionY1).clamp(0)\n",
    "\n",
    "    intersectionArea = intersectionWidth * intersectionHeight\n",
    "\n",
    "    # Union Area\n",
    "    firstWidth, firstHeight = firstBoxX2 - firstBoxX1, firstBoxY2 - firstBoxY1\n",
    "    secondWidth, secondHeight = secondBoxX2 - secondBoxX1, secondBoxY2 - secondBoxY1\n",
    "    unionArea = (firstWidth * firstHeight + 1e-16) + secondWidth * secondHeight - intersectionArea\n",
    "\n",
    "    iou = intersectionArea / unionArea  # iou\n",
    "    \n",
    "    if GIoU:\n",
    "        smallestEnclosingWidth = torch.max(firstBoxX2, secondBoxX2) - torch.min(firstBoxX1, secondBoxX1)  # convex (smallest enclosing box) width\n",
    "        smallestEnclosingHeight = torch.max(firstBoxY2, secondBoxY2) - torch.min(firstBoxY1, secondBoxY1)  # convex height\n",
    "        smallestEnclosingArea = smallestEnclosingWidth * smallestEnclosingHeight + 1e-16  # convex area\n",
    "        return iou - (smallestEnclosingArea - unionArea) / smallestEnclosingArea  # GIoU\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boxIOU() returns the intersection-over-union of the two input boxes. It is important to note that teh two sets of boxes are expected to be in (x1, y1, x2, y2) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxIOU(box1, box2):\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    area1 = (box1.t()[2] - box1.t()[0]) * (box1.t()[3] - box1.t()[1])\n",
    "    area2 = (box2.t()[2] - box2.t()[0]) * (box2.t()[3] - box2.t()[1])\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whIOU() reutns the NxM IoU matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def widthHeightIOU(wh1, wh2):\n",
    "    # Returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2\n",
    "    wh1 = wh1[:, None]  # [N,1,2]\n",
    "    wh2 = wh2[None]  # [1,M,2]\n",
    "    inter = torch.min(wh1, wh2).prod(2)  # [N,M]\n",
    "    return inter / (wh1.prod(2) + wh2.prod(2) - inter)  # iou = inter / (area1 + area2 - inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLosses(p, targets, model):  # predictions, targets, model\n",
    "    FloatTensor = torch.cuda.FloatTensor if p[0].is_cuda else torch.Tensor\n",
    "    classLoss, boxLoss, objectLoss = FloatTensor([0]), FloatTensor([0]), FloatTensor([0])\n",
    "    tcls, tbox, indices, anchors = buildTargets(p, targets, model)  # targets\n",
    "\n",
    "    # Define criteria\n",
    "    BCEcls = nn.BCEWithLogitsLoss(pos_weight = FloatTensor([model.hyp['cls_pw']]), reduction = 'mean')\n",
    "    BCEobj = nn.BCEWithLogitsLoss(pos_weight = FloatTensor([model.hyp['obj_pw']]), reduction = 'mean')\n",
    "\n",
    "    # class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3\n",
    "    cp, cn = 1.0 - 0.5 * 0.0, 0.5 * 0.0\n",
    "\n",
    "    # per output\n",
    "    cumNumTargets = 0  # targets\n",
    "    for layerIdx, layerPrediction in enumerate(p):  # layer index, layer predictions\n",
    "        b, a, gj, gi = indices[layerIdx]  # image, anchor, gridy, gridx\n",
    "        targetObj = torch.zeros_like(layerPrediction[..., 0])  # target obj\n",
    "\n",
    "        numTargets = b.shape[0]  # number of targets\n",
    "        if numTargets:\n",
    "            cumNumTargets += numTargets  # cumulative targets\n",
    "            predictionSubset = layerPrediction[b, a, gj, gi]  # prediction subset corresponding to targets\n",
    "\n",
    "            # GIoU\n",
    "            pxy = predictionSubset[:, :2].sigmoid()\n",
    "            pwh = predictionSubset[:, 2:4].exp().clamp(max = 1E3) * anchors[layerIdx]\n",
    "            pbox = torch.cat((pxy, pwh), 1)  # predicted box\n",
    "            giou = boundingBoxIOU(pbox.t(), tbox[layerIdx], x1y1x2y2 = False, GIoU = True)  # giou(prediction, target)\n",
    "            boxLoss += (1.0 - giou).mean()  # giou loss\n",
    "\n",
    "            # Obj\n",
    "            targetObj[b, a, gj, gi] = (1.0 - model.gr) + model.gr * giou.detach().clamp(0).type(targetObj.dtype)  # giou ratio\n",
    "\n",
    "            # Class\n",
    "            t = torch.full_like(predictionSubset[:, 5:], cn)  # targets\n",
    "            t[range(numTargets), tcls[layerIdx]] = cp\n",
    "            classLoss += BCEcls(predictionSubset[:, 5:], t)  # BCE\n",
    "\n",
    "        objectLoss += BCEobj(layerPrediction[..., 4], targetObj)  # obj loss\n",
    "\n",
    "    boxLoss *= model.hyp['giou']\n",
    "    objectLoss *= model.hyp['obj']\n",
    "    classLoss *= model.hyp['cls']\n",
    "\n",
    "    totLoss = boxLoss + objectLoss + classLoss\n",
    "\n",
    "    return totLoss, torch.cat((boxLoss, objectLoss, classLoss, totLoss)).detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buildTargets is called when computing loss. It works by iterating through the YOLO layers, matching the targets to their anchors and XX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTargets(p, targets, model):\n",
    "    # Build targets for getLosses(), input targets(image,class,x,y,w,h)\n",
    "    numTargets = targets.shape[0]\n",
    "    tcls, tbox, indices, anch = [], [], [], []\n",
    "    gain = torch.ones(6, device = targets.device)  # normalized to gridspace gain\n",
    "\n",
    "    for idx, layer in enumerate(model.yolo_layers):\n",
    "        anchors = model.module_list[layer].anchorVector\n",
    "        gain[2:] = torch.tensor(p[idx].shape)[[3, 2, 3, 2]]  # xyxy gain\n",
    "        numAnchors = anchors.shape[0]  # number of anchors\n",
    "        anchorTensor = torch.arange(numAnchors).view(numAnchors, 1).repeat(1, numTargets)  # anchor tensor, same as .repeat_interleave(nt)\n",
    "\n",
    "        # Match targets to anchors\n",
    "        a, t, offsets = [], targets * gain, 0\n",
    "        if numTargets:\n",
    "\n",
    "            layer = widthHeightIOU(anchors, t[:, 4:6]) > model.hyp['iou_t']  # iou(3,n) = widthHeightIOU(anchors(3,2), gwh(n,2))\n",
    "            a, t = anchorTensor[layer], t.repeat(numAnchors, 1, 1)[layer]  # filter\n",
    "\n",
    "            # overlaps\n",
    "            gxy = t[:, 2:4]  # grid xy\n",
    "\n",
    "       # Define\n",
    "        b, c = t[:, :2].long().T  # image, class\n",
    "        gxy = t[:, 2:4]  # grid xy\n",
    "        gwh = t[:, 4:6]  # grid wh\n",
    "        gij = (gxy - offsets).long()\n",
    "        gi, gj = gij.T  # grid xy indices\n",
    "\n",
    "        # Append\n",
    "        indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices\n",
    "        tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n",
    "        anch.append(anchors[a])  # anchors\n",
    "        tcls.append(c)  # class\n",
    "\n",
    "    return tcls, tbox, indices, anch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMS(prediction, conf_thres = 0.1, iou_thres = 0.6, multi_label = True, classes = None, agnostic = False):\n",
    "    \"\"\"\n",
    "    Performs  Non-Maximum Suppression on inference results\n",
    "    Returns detections with shape:\n",
    "        nx6 (x1, y1, x2, y2, conf, cls)\n",
    "    \"\"\"\n",
    "\n",
    "    # Settings\n",
    "    merge = True  # merge for best mAP\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    time_limit = 10.0  # seconds to quit after\n",
    "\n",
    "    t = time.time()\n",
    "    numClasses = prediction[0].shape[1] - 5  # number of classes\n",
    "    multi_label &= numClasses > 1  # multiple labels per box\n",
    "    output = [None] * prediction.shape[0]\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        x = x[x[:, 4] > conf_thres]  # confidence\n",
    "        x = x[((x[:, 2:4] > min_wh) & (x[:, 2:4] < max_wh)).all(1)]  # width-height\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[..., 5:] *= x[..., 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        i, j = (x[:, 5:] > conf_thres).nonzero().t()\n",
    "        x = torch.cat((box[i], x[i, j + 5].unsqueeze(1), j.float().unsqueeze(1)), 1)\n",
    "\n",
    "        # Filter by class\n",
    "        if classes:\n",
    "            x = x[(j.view(-1, 1) == torch.tensor(classes, device = j.device)).any(1)]\n",
    "\n",
    "        # If none remain process next image\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:\n",
    "            continue\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5] * 0 if agnostic else x[:, 5]  # classes\n",
    "        boxes, scores = x[:, :4].clone() + c.view(-1, 1) * max_wh, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.boxes.nms(boxes, scores, iou_thres)\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            try:  # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "                iou = boxIOU(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "                weights = iou * scores[None]  # box weights\n",
    "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim = True)  # merged boxes\n",
    "            except:  # possible CUDA error https://github.com/ultralytics/yolov3/issues/1139\n",
    "                print(x, i, x.shape, i.shape)\n",
    "                pass\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToTarget(output, width, height):\n",
    "    \"\"\"\n",
    "    Convert a YOLO model output to target format\n",
    "    [batch_id, class_id, x, y, w, h, conf]\n",
    "    \"\"\"\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.cpu().numpy()\n",
    "\n",
    "    targets = []\n",
    "    for i, o in enumerate(output):\n",
    "        if o is not None:\n",
    "            for pred in o:\n",
    "                box = pred[:4]\n",
    "                w = (box[2] - box[0]) / width\n",
    "                h = (box[3] - box[1]) / height\n",
    "                x = box[0] / width + w / 2\n",
    "                y = box[1] / height + h / 2\n",
    "                conf = pred[4]\n",
    "                cls = int(pred[5])\n",
    "\n",
    "                targets.append([i, cls, x, y, w, h, conf])\n",
    "\n",
    "    return np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBox(x, img, color = None, label = None, line_thickness = None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness = tl, lineType = cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale = tl / 3, thickness = tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness = tf, lineType = cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def plotImages(images, targets, paths = None, fname ='images.jpg', names = None, max_size = 640, max_subplots = 16):\n",
    "    tl = 3  # line thickness\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    if os.path.isfile(fname):  # do not overwrite\n",
    "        return None\n",
    "\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.cpu().numpy()\n",
    "\n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.cpu().numpy()\n",
    "\n",
    "    # un-normalise\n",
    "    if np.max(images[0]) <= 1:\n",
    "        images *= 255\n",
    "\n",
    "    bs, _, h, w = images.shape  # batch size, _, height, width\n",
    "    bs = min(bs, max_subplots)  # limit plot images\n",
    "    ns = np.ceil(bs ** 0.5)  # number of subplots (square)\n",
    "\n",
    "    # Check if we should resize\n",
    "    scale_factor = max_size / max(h, w)\n",
    "    if scale_factor < 1:\n",
    "        h = math.ceil(scale_factor * h)\n",
    "        w = math.ceil(scale_factor * w)\n",
    "\n",
    "    # Empty array for output\n",
    "    mosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype = np.uint8)\n",
    "\n",
    "    # Fix class - colour map\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    hex2rgb = lambda h: tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
    "    color_lut = [hex2rgb(h) for h in prop_cycle.by_key()['color']]\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        if i == max_subplots:  # if last batch has fewer images than we expect\n",
    "            break\n",
    "\n",
    "        block_x = int(w * (i // ns))\n",
    "        block_y = int(h * (i % ns))\n",
    "\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        if scale_factor < 1:\n",
    "            img = cv2.resize(img, (w, h))\n",
    "\n",
    "        mosaic[block_y:block_y + h, block_x:block_x + w, :] = img\n",
    "        if len(targets) > 0:\n",
    "            image_targets = targets[targets[:, 0] == i]\n",
    "            boxes = xywh2xyxy(image_targets[:, 2:6]).T\n",
    "            classes = image_targets[:, 1].astype('int')\n",
    "            gt = image_targets.shape[1] == 6  # ground truth if no conf column\n",
    "            conf = None if gt else image_targets[:, 6]  # check for confidence presence (gt vs pred)\n",
    "\n",
    "            boxes[[0, 2]] *= w\n",
    "            boxes[[0, 2]] += block_x\n",
    "            boxes[[1, 3]] *= h\n",
    "            boxes[[1, 3]] += block_y\n",
    "            for j, box in enumerate(boxes.T):\n",
    "                cls = int(classes[j])\n",
    "                color = color_lut[cls % len(color_lut)]\n",
    "                cls = names[cls] if names else cls\n",
    "                if gt or conf[j] > 0.3:  # 0.3 conf thresh\n",
    "                    label = '%s' % cls if gt else '%s %.1f' % (cls, conf[j])\n",
    "                    plotBox(box, mosaic, label = label, color = color, line_thickness = tl)\n",
    "\n",
    "        # Draw image filename labels\n",
    "        if paths is not None:\n",
    "            label = os.path.basename(paths[i])[:40]  # trim to 40 char\n",
    "            t_size = cv2.getTextSize(label, 0, fontScale = tl / 3, thickness = tf)[0]\n",
    "            cv2.putText(mosaic, label, (block_x + 5, block_y + t_size[1] + 5), 0, tl / 3, [220, 220, 220], thickness = tf,\n",
    "                        lineType = cv2.LINE_AA)\n",
    "\n",
    "        # Image border\n",
    "        cv2.rectangle(mosaic, (block_x, block_y), (block_x + w, block_y + h), (255, 255, 255), thickness = 3)\n",
    "\n",
    "    if fname is not None:\n",
    "        mosaic = cv2.resize(mosaic, (int(ns * w * 0.5), int(ns * h * 0.5)), interpolation = cv2.INTER_AREA)\n",
    "        cv2.imwrite(fname, cv2.cvtColor(mosaic, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    return mosaic\n",
    "\n",
    "def plotResults(start = 0, stop = 0, bucket ='', id =()):  \n",
    "    fig, ax = plt.subplots(2, 5, figsize =(12, 6), tight_layout = True)\n",
    "    ax = ax.ravel()\n",
    "    s = ['GIoU', 'Objectness', 'Classification', 'Precision', 'Recall',\n",
    "         'val GIoU', 'val Objectness', 'val Classification', 'mAP@0.5', 'F1']\n",
    "\n",
    "    files = glob.glob('results*.txt') + glob.glob('../../Downloads/results*.txt')\n",
    "    for f in sorted(files):\n",
    "        results = np.loadtxt(f, usecols =[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin = 2).T\n",
    "        n = results.shape[1]  # number of rows\n",
    "        x = range(start, min(stop, n) if stop else n)\n",
    "        for i in range(10):\n",
    "            y = results[i, x]\n",
    "            if i in [0, 1, 2, 5, 6, 7]:\n",
    "                y[y == 0] = np.nan  # dont show zero loss values\n",
    "            ax[i].plot(x, y, marker ='.', label = Path(f).stem, linewidth = 2, markersize = 8)\n",
    "            ax[i].set_title(s[i])\n",
    "\n",
    "    ax[1].legend()\n",
    "    fig.savefig('results.png', dpi = 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  }
 ]
}